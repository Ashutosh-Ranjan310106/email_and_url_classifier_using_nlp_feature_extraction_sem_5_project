{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62665b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911d2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                                url\n",
      "0      0             xenophongroup.com/montjoie/compgns.htm\n",
      "1      1    www.azzali.eu/&usg=AOvVaw2phVSb_ENMrkATGNx5LQ0l\n",
      "2      1                     guildmusic.edu.au/js/index.htm\n",
      "3      1  memo.unexpectedrunner.com/ezxgytw4et\\nholotili...\n",
      "4      0  en.wikipedia.org/wiki/Category:American_televi...\n"
     ]
    }
   ],
   "source": [
    "#dataset1\n",
    "\n",
    "# Load the dataset from Hugging Face Hub\n",
    "train_dataset = load_dataset(\"kmack/Phishing_urls\", split=\"train\")\n",
    "test_dataset = load_dataset(\"kmack/Phishing_urls\", split=\"test\")\n",
    "valid_dataset = load_dataset(\"kmack/Phishing_urls\", split=\"valid\")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "train_df = train_dataset.to_pandas()\n",
    "test_df = test_dataset.to_pandas()\n",
    "valid_df = valid_dataset.to_pandas()\n",
    "\n",
    "\n",
    "all_df = [train_df, test_df,valid_df]\n",
    "for i, df in enumerate(all_df):\n",
    "    df['url'] = df['text']\n",
    "    df.drop('text',axis=1,inplace=True)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2083a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 99\n",
      "Sample: ['<PAD>', '<UNK>', '<START>', '<END>', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "special_tokens = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "\n",
    "# ASCII printable characters\n",
    "ascii_chars = [chr(i) for i in range(32, 127)]  # ' ' (space) to '~'\n",
    "\n",
    "# Full vocabulary\n",
    "vocab = special_tokens + ascii_chars\n",
    "\n",
    "# Create mappings\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"Sample:\", vocab[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00707761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567056/567056 [00:04<00:00, 117008.05it/s]\n",
      "100%|██████████| 70882/70882 [00:00<00:00, 112901.87it/s]\n",
      "100%|██████████| 70882/70882 [00:00<00:00, 109390.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "max_url_length = 200\n",
    "\n",
    "\n",
    "def encode_url(url):\n",
    "    encoded = np.ones(max_url_length, dtype=int) * char2idx['<PAD>']  # padding value\n",
    "    for i, ch in enumerate(url[:max_url_length]):\n",
    "        encoded[i] = char2idx.get(ch, char2idx['<UNK>'])\n",
    "    return encoded\n",
    "\n",
    "# Apply encoding to your URL column\n",
    "train_df['encode'] = train_df['url'].progress_apply(encode_url)\n",
    "test_df['encode'] = test_df['url'].progress_apply(encode_url)\n",
    "valid_df['encode'] = valid_df['url'].progress_apply(encode_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a7baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567056/567056 [00:01<00:00, 336802.40it/s]\n",
      "100%|██████████| 70882/70882 [00:00<00:00, 334348.10it/s]\n",
      "100%|██████████| 70882/70882 [00:00<00:00, 385250.19it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "train_df['encode'] = train_df['encode'].progress_apply(lambda enc: enc / vocab_size)\n",
    "test_df['encode'] = test_df['encode'].progress_apply(lambda enc: enc / vocab_size)\n",
    "valid_df['encode'] = valid_df['encode'].progress_apply(lambda enc: enc / vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b67d8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.9292929292929293, 0.7373737373737373, 0.828...\n",
       "1    [0.9191919191919192, 0.9191919191919192, 0.919...\n",
       "2    [0.7575757575757576, 0.898989898989899, 0.7777...\n",
       "3    [0.8181818181818182, 0.7373737373737373, 0.818...\n",
       "4    [0.7373737373737373, 0.8282828282828283, 0.181...\n",
       "Name: encode, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.encode.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce1263c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert encoded URLs to proper tensors\n",
    "url_tensor = torch.tensor(np.stack(train_df['encode'].values), dtype=torch.long)\n",
    "labels_tensor = torch.tensor(train_df['label'].values, dtype=torch.long)\n",
    "\n",
    "valid_url_tensor = torch.tensor(np.stack(valid_df['encode'].values), dtype=torch.long)\n",
    "valid_labels_tensor = torch.tensor(valid_df['label'].values, dtype=torch.long)\n",
    "\n",
    "test_url_tensor = torch.tensor(np.stack(test_df['encode'].values), dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_df['label'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(url_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(valid_url_tensor, valid_labels_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_url_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87e5f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class URLBinaryCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, num_classes=1, max_len=200):\n",
    "        super(URLBinaryCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=char2idx['<PAD>']\n",
    "        )\n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AvgPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(64 * (max_len // 2), num_classes)  # after one pooling\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x = self.embedding(x)           # [batch, seq_len, embed_dim]\n",
    "        x = x.permute(0, 2, 1)         # [batch, embed_dim, seq_len] for conv1d\n",
    "        x = F.relu(self.conv1(x))      \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)               # downsample by 2\n",
    "        x = x.view(x.size(0), -1)      # flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)            # output probability\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fc52bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.6932, Train Acc: 0.4998 | Val Loss: 0.6932, Val Acc: 0.4988\n",
      "Epoch 2/5 | Train Loss: 0.6932, Train Acc: 0.5000 | Val Loss: 0.6932, Val Acc: 0.4988\n",
      "Epoch 3/5 | Train Loss: 0.6931, Train Acc: 0.4998 | Val Loss: 0.6932, Val Acc: 0.4988\n",
      "Epoch 4/5 | Train Loss: 0.6931, Train Acc: 0.5004 | Val Loss: 0.6932, Val Acc: 0.4988\n",
      "Epoch 5/5 | Train Loss: 0.6932, Train Acc: 0.5001 | Val Loss: 0.6931, Val Acc: 0.5012\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = URLBinaryCNN(vocab_size=len(vocab), embed_dim=32).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        correct_train += (preds == batch_y).sum().item()\n",
    "        total_train += batch_x.size(0)\n",
    "    \n",
    "    train_acc = correct_train / total_train\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            correct_val += (preds == batch_y).sum().item()\n",
    "            total_val += batch_x.size(0)\n",
    "    \n",
    "    val_acc = correct_val / total_val\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss/total_train:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss/total_val:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b199b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f92664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_url_tensor), 64):  # batch size 64\n",
    "        batch_x = test_url_tensor[i:i+64].to(device)\n",
    "        batch_y = test_labels_tensor[i:i+64].to(device)\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        preds = (outputs >= 0.5).long().squeeze(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6193c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Optional: pretty plot\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(all_labels, all_preds, target_names=['Safe', 'Phishing']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
