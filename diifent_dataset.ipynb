{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2ae588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                url\n",
      "0     2                        https://blog.sockpuppet.us/\n",
      "1     2                  https://blog.apiki.com/seguranca/\n",
      "2     1  http://autoecole-lauriston.com/a/T0RVd056QXlNe...\n",
      "3     1  http://chinpay.site/index.html?hgcFSE@E$Z*DFcG...\n",
      "4     2  http://www.firstfivenebraska.org/blog/article/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:70: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df4['label'][df4['label'] == 1] = 0\n",
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:71: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df4['label'][df4['label'] == 2] = 1\n",
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['label'].apply(lambda x: 1 if x in phishing_labels else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset 1 (Malicious URLs) cleaned: 522214 samples (Phishing=94111, Benign=428103)\n",
      "\n",
      "ðŸ“‚ Dataset 1 (Malicious URLs) split â†’ Train: 417732, Valid: 52217, Test: 52217\n",
      "\n",
      "ðŸ“‚ Dataset 1 (Malicious URLs) split â†’ Train: 188296, Valid: 23537, Test: 23537\n",
      "\n",
      "ðŸ“‚ Dataset 1 (Malicious URLs) split â†’ Train: 528097, Valid: 66012, Test: 66013\n",
      "\n",
      "ðŸ“‚ Dataset 1 (Malicious URLs) split â†’ Train: 639999, Valid: 80000, Test: 80000\n",
      "âœ… Seed fixed to 42\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "from urllib.parse import urlparse\n",
    "from three_datasets import all_dataset\n",
    "import random\n",
    "tqdm.pandas()\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ… Seed fixed to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "You are using a model of type canine to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,  T5EncoderModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"google/canine-s\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5EncoderModel.from_pretrained(model_name)  \n",
    "model.eval()\n",
    "def get_byt5_embedding(url):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(url, return_tensors=\"pt\", truncation=True, padding=True, max_length=50)\n",
    "    \n",
    "    # Get encoder output (not generate)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # outputs.last_hidden_state: [batch_size, seq_len, hidden_dim]\n",
    "         \n",
    "        # Mean pooling across sequence\n",
    "        emb = outputs.last_hidden_state.mean(dim=1).reshape(-1)\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "# Example\n",
    "#url = \"http://paypal-login-secure-update.com\"\n",
    "#vector = get_byt5_embedding(url)\n",
    "#print(\"Embedding shape:\", vector.shape)  # e.g. (512,) or (768,) depending on model size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496a3fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load once globally (so it doesn't reload every call)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"google/canine-s\"  # or \"google/canine-c\" (larger, slower)\n",
    "\n",
    "# Load model + tokenizer only once\n",
    "tokenizer_canine = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_canine = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model_canine.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_canine_embedding(url, max_length=50, pooling=\"mean\"):\n",
    "    \"\"\"\n",
    "    Returns a fixed-length CANINE embedding for a single URL string.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL text to encode.\n",
    "        max_length (int): Maximum length of character sequence (truncate/pad to this).\n",
    "        pooling (str): 'mean' or 'max' for pooling token embeddings into one vector.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1D embedding vector (shape [hidden_dim], e.g. [768]).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Tokenize\n",
    "    encoded = tokenizer_canine(\n",
    "        url,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model_canine(**encoded)\n",
    "    hidden = outputs.last_hidden_state  # [1, seq_len, hidden_dim]\n",
    "    mask = encoded[\"attention_mask\"].unsqueeze(-1).type_as(hidden)  # [1, seq_len, 1]\n",
    "\n",
    "    # Pooling\n",
    "    if pooling == \"mean\":\n",
    "        summed = (hidden * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = summed / denom  # [1, hidden_dim]\n",
    "    elif pooling == \"max\":\n",
    "        hidden = hidden.masked_fill(mask == 0, -1e9)\n",
    "        pooled = hidden.max(dim=1).values  # [1, hidden_dim]\n",
    "    else:\n",
    "        raise ValueError(\"Pooling must be 'mean' or 'max'\")\n",
    "\n",
    "    return pooled.squeeze(0).cpu().numpy()  # (hidden_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357bf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”  Encoding structured URLs for dataset_0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417732/417732 [01:16<00:00, 5494.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… train: Encoded 417732 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:10<00:00, 5121.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… valid: Encoded 52217 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:09<00:00, 5490.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… test: Encoded 52217 URLs\n",
      "\n",
      "âœ… All datasets encoded with proper start/end markers and padding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Character Encoding Setup\n",
    "# ============================================================\n",
    "\n",
    "# Allowed printable ASCII chars\n",
    "ascii_chars = [chr(i) for i in range(32, 127)]\n",
    "\n",
    "# Special control tokens\n",
    "special_tokens = [\n",
    "    '<PAD>', '<UNK>',\n",
    "]\n",
    "\n",
    "# Build vocab and mapping\n",
    "vocab = special_tokens + ascii_chars\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "\n",
    "\n",
    "def encode(text, max_len=100):\n",
    "    indices = torch.full((max_len,), char2idx[\"<PAD>\"], dtype=torch.long)\n",
    "    text = text.lower()[:max_len]\n",
    "    for i, c in enumerate(text):\n",
    "        indices[i] = char2idx.get(c, char2idx[\"<UNK>\"])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoded_data = {}\n",
    "frac = 1\n",
    "gen = all_dataset()\n",
    "\n",
    "\n",
    "\n",
    "for i, splits in zip([f'dataset_{i}' for i in range(4)], gen):\n",
    "    encoded_data[i] = {}\n",
    "    print(f\"\\nðŸ”  Encoding structured URLs for {i}...\")\n",
    "\n",
    "    for split_name, df in zip(['train', 'valid', 'test'], splits):\n",
    "        df = df.sample(frac=frac, random_state=42)\n",
    "        df[\"encode\"] = df[\"url\"].progress_apply(encode)\n",
    "        encoded_data[i][split_name] = df\n",
    "        print(f\"  âœ… {split_name}: Encoded {len(df)} URLs\")\n",
    "    next(gen)\n",
    "    next(gen)\n",
    "    next(gen)\n",
    "\n",
    "print(\"\\nâœ… All datasets encoded with proper start/end markers and padding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eeebb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Creating DataLoaders for dataset_0...\n",
      "âœ… DataLoaders ready for dataset_0 (Train/Val/Test)\n",
      "\n",
      "ðŸš€ All DataLoaders are ready in `dataloader_dict`!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# ============================================================\n",
    "# Convert to TensorDataset and DataLoader\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "\n",
    "dataloader_dict = {}\n",
    "\n",
    "def make_tensor_dataset(df):\n",
    "    url_tensor = torch.stack(list(df[\"encode\"]))\n",
    "    labels_tensor = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "    return TensorDataset(url_tensor, labels_tensor)\n",
    "\n",
    "for name, splits in encoded_data.items():\n",
    "    dataloader_dict[name] = {}\n",
    "    print(f\"\\nðŸ“¦ Creating DataLoaders for {name}...\")\n",
    "    \n",
    "    train_set = make_tensor_dataset(splits[\"train\"])\n",
    "    val_set = make_tensor_dataset(splits[\"valid\"])\n",
    "    test_set = make_tensor_dataset(splits[\"test\"])\n",
    "    \n",
    "    dataloader_dict[name][\"train_loader\"] = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    dataloader_dict[name][\"val_loader\"] = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    dataloader_dict[name][\"test_loader\"] = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"âœ… DataLoaders ready for {name} (Train/Val/Test)\")\n",
    "\n",
    "print(\"\\nðŸš€ All DataLoaders are ready in `dataloader_dict`!\")\n",
    "\n",
    "# Example Access:\n",
    "# dataloader_dict[\"dataset1\"][\"train_loader\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ea480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ TinyByT5 Encoder (Reduced depth for short sequences)\n",
    "# =====================================================\n",
    "class TinyByT5Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=256,\n",
    "                 d_model=128,\n",
    "                 num_layers=2,\n",
    "                 num_heads=2,\n",
    "                 dim_ff=256,\n",
    "                 max_len=100,\n",
    "                 n_out=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ðŸ”¹ Byte embedding layer (0â€“255)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # ðŸ”¹ Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # ðŸ”¹ Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # ðŸ”¹ Final normalization\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.projection = nn.Linear(in_features=d_model, out_features=n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) â€” byte indices [0â€“255]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "        device = x.device\n",
    "\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.projection(self.final_norm(x))\n",
    "\n",
    "        return x  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ Tiny Transformer Encoder\n",
    "# ==========================\n",
    "class TinyURLTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=2, num_layers=2, ff_dim=128, max_len=100, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\"\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.projection = nn.Linear(embed_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        #x = self.embedding(x) + self.pos_embedding[:, :x.size(1), :]  # (B, L, E)\n",
    "        #x = self.transformer(x)                                       # (B, L, E)\n",
    "        return self.embedding(x) #self.projection(x)                                     # (B, L, out_dim)\n",
    "\n",
    "class URLBinaryCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, maxlen=100):\n",
    "        super(URLBinaryCNN, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.transformer = TinyByT5Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=maxlen,\n",
    "            d_model=512,\n",
    "            n_out=embed_dim\n",
    "        )\n",
    "        # ðŸ”¹ 1st Conv block\n",
    "        self.conv1_3x3 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.convreduce1_1x1 = nn.Conv1d(in_channels=embed_dim, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.conv1_5x5 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv1_7 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv1_1x1 = nn.Conv1d(in_channels=32*3, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=[32, maxlen])\n",
    "\n",
    "\n",
    "         # ðŸ”¹ 2st Conv block\n",
    "        self.conv2_3x3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.convreduce2_1x1 = nn.Conv1d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.conv2_5x5 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv2_1x1 = nn.Conv1d(in_channels=64*2, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=[64, maxlen])\n",
    "        \n",
    "\n",
    "         # ðŸ”¹ 3st Conv block\n",
    "        self.conv3_3x3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.convreduce3_1x1 = nn.Conv1d(in_channels=64, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.conv3_5x5 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv3_7 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv3_1x1 = nn.Conv1d(in_channels=32*3, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.layer_norm3 = nn.LayerNorm(normalized_shape=[32, maxlen])\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=32, num_layers=1, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.aap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # ðŸ”¹ Fully connected layers\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len)\n",
    "        x = self.transformer(x)           # (B, L, E)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)            # (B, E, L)\n",
    "\n",
    "\n",
    "\n",
    "        # ðŸ”¹ Block 1\n",
    "        x3 = F.relu(self.conv1_3x3(x))\n",
    "\n",
    "        #xr = F.relu(self.convreduce1_1x1(x))\n",
    "        x5 = F.relu(self.conv1_5x5(x))\n",
    "        x7 = F.relu(self.conv1_7(x))\n",
    "\n",
    "        x = torch.cat([x3, x5, x7], dim=1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1_1x1(x))\n",
    "\n",
    "        #x = self.layer_norm1(x)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # ðŸ”¹ Block 2\n",
    "        x3 = F.relu(self.conv2_3x3(x))\n",
    "        xr = F.relu(self.convreduce2_1x1(x))\n",
    "        x5 = F.relu(self.conv2_5x5(xr))\n",
    "        x = torch.cat([x3, x5], dim=1)\n",
    "        x = F.relu(self.conv2_1x1(x))\n",
    "        x = self.layer_norm2(x)\n",
    "        '''\n",
    "        # ðŸ”¹ Block 3\n",
    "        #x = F.relu(self.conv3_3x3(x))\n",
    "        #xr = F.relu(self.convreduce3_1x1(x))\n",
    "        #x5 = F.relu(self.conv3_5x5(x))\n",
    "        #x7 = F.relu(self.conv3_7(x))\n",
    "        #x = torch.cat([x3, x5, x7], dim=1)\n",
    "        #x = F.relu(self.conv3_1x1(x))\n",
    "        #x = self.layer_norm3(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        # ðŸ”¹ Global Average Pooling + FC\n",
    "        #x = self.aap(x)                   # (batch, channels, 1)\n",
    "        x = self.flatten(x)               # (batch, channels)\n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64290975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 criterion, \n",
    "                 transformer_optimizer=None,\n",
    "                 main_optimizer = None, \n",
    "                 scheduler_t=None,\n",
    "                 scheduler_c=None,\n",
    "                 train_loader=None, \n",
    "                 val_loader=None,   \n",
    "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \n",
    "        \"\"\"\n",
    "        optimizer_groups: dict with keys like {\"transformer\": optimizer1, \"cnn\": optimizer2}\n",
    "        schedulers: dict with keys matching optimizer_groups (optional)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.transformer_params = []\n",
    "        self.cnn_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(\"transformer.\"):\n",
    "                self.transformer_params.append(param)\n",
    "            else:\n",
    "                self.cnn_params.append(param)\n",
    "\n",
    "\n",
    "\n",
    "        self.transformer_optimizer = optim.Adam(self.transformer_params, lr=1e-4) if transformer_optimizer is None  else transformer_optimizer\n",
    "        self.main_optimizer = optim.Adam(self.cnn_params, lr=1e-3) if main_optimizer is None else main_optimizer\n",
    "        self.scheduler_t = optim.lr_scheduler.ReduceLROnPlateau(self.transformer_optimizer, mode='min', factor=0.5, patience=2) if scheduler_t is None  else scheduler_t\n",
    "        self.scheduler_c = optim.lr_scheduler.ReduceLROnPlateau(self.main_optimizer, mode='min', factor=0.5, patience=2) if scheduler_c is None  else scheduler_c\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_losses, self.val_losses = [], []\n",
    "        self.train_accs, self.val_accs = [], []\n",
    "\n",
    "\n",
    "    def freeze_module(self, module):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_module(self, module):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def train(self, epochs_list=[3,3,4], early_stopping=True, frac=1.0, val_frac=1.0, alt_cycle = 2,  log=False):\n",
    "        for phase, epochs in enumerate(epochs_list):\n",
    "            for epoch in range(epochs):\n",
    "                if phase == 0:\n",
    "                    # ðŸ§  Train Transformer â€” freeze CNN/LSTM layers\n",
    "                    self.unfreeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        if name != \"transformer\":\n",
    "                            self.freeze_module(module)\n",
    "                        else:\n",
    "                            self.unfreeze_module(module)\n",
    "                    active_optims = [self.transformer_optimizer]\n",
    "                    active_scheds = [self.scheduler_t]\n",
    "                    phase_name = \"Transformer\"\n",
    "                elif phase == 1:\n",
    "                    # ðŸŽ¯ Train CNN/LSTM/FC â€” freeze Transformer\n",
    "                    self.freeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        if name == \"transformer\":\n",
    "                            self.freeze_module(module)\n",
    "                        else:\n",
    "                            self.unfreeze_module(module)\n",
    "                    active_optims = [self.main_optimizer]\n",
    "                    active_scheds = [self.scheduler_c]\n",
    "                    phase_name = \"CNN\"\n",
    "                else:\n",
    "                    self.freeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        self.unfreeze_module(module)\n",
    "                    active_optims = [self.transformer_optimizer, self.main_optimizer]\n",
    "                    active_scheds = [self.scheduler_t, self.scheduler_c]\n",
    "                    phase_name = \"CNN + transformer\"\n",
    "\n",
    "\n",
    "\n",
    "                self.model.train()\n",
    "                train_loss, correct_train, total_train = 0, 0, 0\n",
    "                max_batches = int(len(self.train_loader) * frac)\n",
    "                \n",
    "                for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
    "                    if batch_idx >= max_batches:\n",
    "                        break\n",
    "                    \n",
    "                    batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True).float().unsqueeze(1)\n",
    "                    \n",
    "                    for opt in active_optims:\n",
    "                        opt.zero_grad()\n",
    "\n",
    "                    outputs = self.model(batch_x)\n",
    "                    loss = self.criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "\n",
    "                    for opt in active_optims:\n",
    "                        opt.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # === Metrics ===\n",
    "                    batch_loss = loss.item()\n",
    "                    preds = (outputs >= 0.5).float()\n",
    "                    batch_acc = (preds == batch_y).float().mean().item()\n",
    "\n",
    "                    train_loss += batch_loss * batch_x.size(0)\n",
    "                    correct_train += (preds == batch_y).sum().item()\n",
    "                    total_train += batch_x.size(0)\n",
    "\n",
    "                    if log and (batch_idx + 1) % 20 == 0:\n",
    "                        print(f\"\\rEpoch {epoch+1}/{epochs}: Training {phase_name} | \"\n",
    "                            f\"Batch {batch_idx+1}/{max_batches} | \"\n",
    "                            f\"Loss: {batch_loss:.4f}, Acc: {batch_acc:.4f}\", end='')\n",
    "\n",
    "                avg_train_loss = train_loss / total_train\n",
    "                train_acc = correct_train / total_train\n",
    "                self.train_losses.append(avg_train_loss)\n",
    "                self.train_accs.append(train_acc)\n",
    "\n",
    "                # === Validation ===\n",
    "                if self.val_loader is not None:\n",
    "                    avg_val_loss, val_acc = self.evaluate(val_frac)\n",
    "                    self.val_losses.append(avg_val_loss)\n",
    "                    self.val_accs.append(val_acc)\n",
    "\n",
    "                    if log:\n",
    "                        print(f\"\\rEpoch {epoch+1}/{epochs} Training {phase_name}| \"\n",
    "                            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                elif log:\n",
    "                    print(f\"\\rEpoch {epoch+1}/{epochs} Training {phase_name}| \"\n",
    "                        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "                for sched in active_scheds:\n",
    "                    sched.step(avg_val_loss)\n",
    "\n",
    "\n",
    "\n",
    "        def evaluate(self, frac=1.0):\n",
    "            self.model.eval()\n",
    "            val_loss, correct_val, total_val = 0, 0, 0\n",
    "            max_batches = max(int(len(self.val_loader) * frac), 1)\n",
    "            print(f'\\r total validation batch size {max_batches}'.ljust(100), end='')\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (batch_x, batch_y) in enumerate(self.val_loader):\n",
    "                    if batch_idx >= max_batches:\n",
    "                        break\n",
    "                    batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True).float().unsqueeze(1)\n",
    "                    outputs = self.model(batch_x)\n",
    "                    loss = self.criterion(outputs, batch_y)\n",
    "                    avg_batch_loss = loss.item()\n",
    "                    val_loss += avg_batch_loss * batch_x.size(0)\n",
    "                    preds = (outputs >= 0.5).float()\n",
    "                    correct_val += (preds == batch_y).sum().item()\n",
    "                    total_val += batch_x.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / total_val\n",
    "            val_acc = correct_val / total_val\n",
    "            return avg_val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dcb57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transformer']\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = URLBinaryCNN(vocab_size=len(vocab)).to(device)\n",
    "transformer_params = []\n",
    "cnn_params = []\n",
    "for name, module in model.named_children():\n",
    "    if name == \"transformer\":  # all encoder weights\n",
    "        transformer_params.append(name)\n",
    "    else:  # everything else (cnn, lstm, fc, etc.)\n",
    "        cnn_params.append(name)\n",
    "print(transformer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14896532",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Training model on DATASET_0 dataset\n",
      "======================================================================\n",
      "___________ðŸ§© Using 50% of training data___________\n",
      "Epoch 1/10 Training Transformer| Train Loss: 0.6369, Train Acc: 0.8195 | Val Loss: 0.6318, Val Acc: 0.8200\n",
      "Epoch 2/10 Training Transformer| Train Loss: 0.6163, Train Acc: 0.8630 | Val Loss: 0.6028, Val Acc: 0.9468\n",
      "Epoch 3/10 Training CNN| Train Loss: 0.2309, Train Acc: 0.9438 | Val Loss: 0.1833, Val Acc: 0.9475 \n",
      "Epoch 4/10 Training CNN| Train Loss: 0.1942, Train Acc: 0.9443 | Val Loss: 0.1611, Val Acc: 0.9467 \n",
      "Epoch 5/10 Training Transformer| Train Loss: 0.1821, Train Acc: 0.9406 | Val Loss: 0.1545, Val Acc: 0.9438\n",
      "Epoch 6/10 Training Transformer| Train Loss: 0.1687, Train Acc: 0.9431 | Val Loss: 0.1401, Val Acc: 0.9492\n",
      "Epoch 7/10 Training CNN| Train Loss: 0.1597, Train Acc: 0.9481 | Val Loss: 0.1348, Val Acc: 0.9476 \n",
      "Epoch 8/10 Training CNN| Train Loss: 0.1555, Train Acc: 0.9513 | Val Loss: 0.1291, Val Acc: 0.9580 \n",
      "Epoch 9/10 Training CNN + transformer| Train Loss: 0.1476, Train Acc: 0.9539 | Val Loss: 0.1410, Val Acc: 0.9542\n",
      "Epoch 10/10 Training CNN + transformer| Train Loss: 0.1375, Train Acc: 0.9568 | Val Loss: 0.1182, Val Acc: 0.9631\n",
      "__________ðŸ§© Using 100% of training data___________\n",
      "Epoch 1/10 Training Transformer| Train Loss: 0.6992, Train Acc: 0.4022 | Val Loss: 0.6946, Val Acc: 0.1802\n",
      "Epoch 2/10 Training Transformer| Train Loss: 0.6905, Train Acc: 0.5516 | Val Loss: 0.6844, Val Acc: 0.8198\n",
      "Epoch 3/10 Training CNN| Train Loss: 0.4828, Train Acc: 0.8189 | Val Loss: 0.4721, Val Acc: 0.8198 \n",
      "Epoch 4/10 Training CNN| Train Loss: 0.4723, Train Acc: 0.8198 | Val Loss: 0.4717, Val Acc: 0.8198 \n",
      "Epoch 5/10 Training Transformer| Train Loss: 0.4717, Train Acc: 0.8198 | Val Loss: 0.4717, Val Acc: 0.8198\n",
      "Epoch 6/10 Training Transformer| Train Loss: 0.4714, Train Acc: 0.8198 | Val Loss: 0.4659, Val Acc: 0.8198\n",
      "Epoch 7/10 Training CNN| Train Loss: 0.3091, Train Acc: 0.8787 | Val Loss: 0.2605, Val Acc: 0.9081 \n",
      "Epoch 8/10 Training CNN| Train Loss: 0.2551, Train Acc: 0.9177 | Val Loss: 0.1973, Val Acc: 0.9365 \n",
      "Epoch 9/10 Training CNN + transformer| Train Loss: 0.1919, Train Acc: 0.9379 | Val Loss: 0.1270, Val Acc: 0.9601\n",
      "Epoch 10/10 Training CNN + transformer| Train Loss: 0.1607, Train Acc: 0.9487 | Val Loss: 0.1146, Val Acc: 0.9626\n",
      "\n",
      "âœ… All datasets trained successfully!\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ Final Validation Accuracy Summary\n",
      "======================================================================\n",
      "dataset_0            | Val Acc: 0.9626 | Val Loss: 0.1146\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ Training Config\n",
    "# ============================================================\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Store all dataset metrics\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ” Training Loop for Each Dataset\n",
    "# ============================================================\n",
    "for dataset_name, loaders in {'dataset_0':dataloader_dict['dataset_0']}.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training model on {dataset_name.upper()} dataset\")\n",
    "    print(\"=\"*70)\n",
    "    for frac in  [0.5, 1.0]:\n",
    "        print(f\"ðŸ§© Using {frac*100:.0f}% of training data\".center(50, '_'))\n",
    "        train_loader = loaders[\"train_loader\"]\n",
    "        val_loader = loaders[\"val_loader\"]\n",
    "\n",
    "        # Initialize model, loss, optimizer\n",
    "        model = URLBinaryCNN(vocab_size=len(vocab)).to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lr/10)\n",
    "        trainer = Train(model, criterion, train_loader=train_loader, val_loader=val_loader)\n",
    "        # Lists to track performance\n",
    "        trainer.train(num_epochs,lr,frac=frac,val_frac=frac, log=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Store all results for this dataset\n",
    "    all_results[dataset_name] = {\n",
    "        \"train_losses\": trainer.train_losses,\n",
    "        \"val_losses\": trainer.val_losses,\n",
    "        \"train_accs\": trainer.train_accs,\n",
    "        \"val_accs\": trainer.val_accs,\n",
    "        \"final_val_acc\": trainer.val_accs[-1],\n",
    "        \"final_val_loss\": trainer.val_losses[-1]\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… All datasets trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š Summary of All Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ Final Validation Accuracy Summary\")\n",
    "print(\"=\"*70)\n",
    "for name, res in all_results.items():\n",
    "    print(f\"{name:<20} | Val Acc: {res['final_val_acc']:.4f} | Val Loss: {res['final_val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Training model on DATASET_0 dataset\n",
      "======================================================================\n",
      "___________ðŸ§© Using 10% of training data___________\n",
      "Epoch 1/6 | Train Loss: 0.2614, Train Acc: 0.9133 | Val Loss: 0.0198, Val Acc: 0.9978              \n",
      "Epoch 2/6 | Train Loss: 0.1030, Train Acc: 0.9702 | Val Loss: 0.0159, Val Acc: 0.9978              \n",
      "Epoch 3/6 | Train Loss: 0.0480, Train Acc: 0.9870 | Val Loss: 0.0151, Val Acc: 0.9978              \n",
      "Epoch 4/6 | Train Loss: 0.0409, Train Acc: 0.9914 | Val Loss: 0.0154, Val Acc: 0.9974              \n",
      "Epoch 5/6 | Train Loss: 0.0443, Train Acc: 0.9895 | Val Loss: 0.0148, Val Acc: 0.9978              \n",
      "Epoch 6/6 | Train Loss: 0.0383, Train Acc: 0.9912 | Val Loss: 0.0150, Val Acc: 0.9978              \n",
      "___________ðŸ§© Using 50% of training data___________\n",
      "Epoch 1/6 | Train Loss: 0.0871, Train Acc: 0.9817 | Val Loss: 0.0160, Val Acc: 0.9975              \n",
      "Epoch 2/6 | Train Loss: 0.0614, Train Acc: 0.9895 | Val Loss: 0.0132, Val Acc: 0.9980              \n",
      "Epoch 3/6 | Train Loss: 0.0356, Train Acc: 0.9935 | Val Loss: 0.0138, Val Acc: 0.9977              \n",
      "Epoch 4/6 | Train Loss: 0.0305, Train Acc: 0.9945 | Val Loss: 0.0133, Val Acc: 0.9980              \n",
      "Epoch 5/6 | Train Loss: 0.0299, Train Acc: 0.9946 | Val Loss: 0.0135, Val Acc: 0.9978              \n",
      "Epoch 6/6 | Train Loss: 0.0276, Train Acc: 0.9942 | Val Loss: 0.0119, Val Acc: 0.9980              \n",
      "__________ðŸ§© Using 100% of training data___________\n",
      "Epoch 1/6 | Train Loss: 0.0755, Train Acc: 0.9817 | Val Loss: 0.0146, Val Acc: 0.9977              \n",
      "Epoch 2/6 |  Batch 820/1472 |  Loss: 0.0413, Acc: 0.9844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     trainer = Train(model, criterion, optimizer, train_loader, val_loader)\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Lists to track performance\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfrac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrac\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Store all results for this dataset\u001b[39;00m\n\u001b[32m     24\u001b[39m all_results[dataset_name] = {\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_losses\u001b[39m\u001b[33m\"\u001b[39m: trainer.train_losses,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_losses\u001b[39m\u001b[33m\"\u001b[39m: trainer.val_losses,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfinal_val_loss\u001b[39m\u001b[33m\"\u001b[39m: trainer.val_losses[-\u001b[32m1\u001b[39m]\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mTrain.train\u001b[39m\u001b[34m(self, epochs, lr, early_stopping, frac, val_frac, log)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m batch_loss = \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m preds = (outputs >= \u001b[32m0.5\u001b[39m).float()\n\u001b[32m     30\u001b[39m batch_acc = (preds == batch_y).float().mean().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# ðŸ” Training Loop for Each Dataset\n",
    "# ============================================================\n",
    "for dataset_name, loaders in {'dataset_0':dataloader_dict['dataset_0']}.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training model on {dataset_name.upper()} dataset\")\n",
    "    print(\"=\"*70)\n",
    "    for frac in  [0.5, 1.0]:\n",
    "        print(f\"ðŸ§© Using {frac*100:.0f}% of training data\".center(50, '_'))\n",
    "        train_loader = loaders[\"train_loader\"]\n",
    "        val_loader = loaders[\"val_loader\"]\n",
    "\n",
    "        # Initialize model, loss, optimizer\n",
    "        model = URLBinaryCNN(vocab_size=len(vocab)).to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lr/10)\n",
    "        trainer = Train(model, criterion, optimizer, train_loader, val_loader)\n",
    "        # Lists to track performance\n",
    "        trainer.train(num_epochs,lr,frac=frac,val_frac=frac, log=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Store all results for this dataset\n",
    "    all_results[dataset_name] = {\n",
    "        \"train_losses\": trainer.train_losses,\n",
    "        \"val_losses\": trainer.val_losses,\n",
    "        \"train_accs\": trainer.train_accs,\n",
    "        \"val_accs\": trainer.val_accs,\n",
    "        \"final_val_acc\": trainer.val_accs[-1],\n",
    "        \"final_val_loss\": trainer.val_losses[-1]\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… All datasets trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š Summary of All Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ Final Validation Accuracy Summary\")\n",
    "print(\"=\"*70)\n",
    "for name, res in all_results.items():\n",
    "    print(f\"{name:<20} | Val Acc: {res['final_val_acc']:.4f} | Val Loss: {res['final_val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e6e11",
   "metadata": {},
   "source": [
    "\n",
    "======================================================================  \n",
    "ðŸš€ Training model on DATASET_2 dataset\n",
    "======================================================================  \n",
    "___________ðŸ§© Using 10% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.6624, Train Acc: 0.6061 | Val Loss: 0.5799, Val Acc: 0.7219                \n",
    "Epoch 2/6 | Train Loss: 0.5179, Train Acc: 0.7594 | Val Loss: 0.5534, Val Acc: 0.7312                \n",
    "Epoch 3/6 | Train Loss: 0.4341, Train Acc: 0.8096 | Val Loss: 0.4320, Val Acc: 0.8120                \n",
    "Epoch 4/6 | Train Loss: 0.3945, Train Acc: 0.8308 | Val Loss: 0.4003, Val Acc: 0.8252              \n",
    "Epoch 5/6 | Train Loss: 0.3716, Train Acc: 0.8404 | Val Loss: 0.3567, Val Acc: 0.8496              \n",
    "Epoch 6/6 | Train Loss: 0.3588, Train Acc: 0.8458 | Val Loss: 0.3478, Val Acc: 0.8523              \n",
    "___________ðŸ§© Using 50% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.5106, Train Acc: 0.7353 | Val Loss: 0.5287, Val Acc: 0.6781              \n",
    "Epoch 2/6 | Train Loss: 0.3578, Train Acc: 0.8452 | Val Loss: 0.5693, Val Acc: 0.6529              \n",
    "Epoch 3/6 | Train Loss: 0.3324, Train Acc: 0.8569 | Val Loss: 0.5073, Val Acc: 0.7133              \n",
    "Epoch 4/6 | Train Loss: 0.3191, Train Acc: 0.8620 | Val Loss: 0.5989, Val Acc: 0.6296              \n",
    "Epoch 5/6 | Train Loss: 0.3086, Train Acc: 0.8663 | Val Loss: 0.4869, Val Acc: 0.7649              \n",
    "Epoch 6/6 | Train Loss: 0.3027, Train Acc: 0.8693 | Val Loss: 0.4661, Val Acc: 0.7953              \n",
    "__________ðŸ§© Using 100% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.4213, Train Acc: 0.8050 | Val Loss: 0.3473, Val Acc: 0.8572              \n",
    "Epoch 2/6 | Train Loss: 0.3203, Train Acc: 0.8626 | Val Loss: 0.3535, Val Acc: 0.8393              \n",
    "Epoch 3/6 | Train Loss: 0.3016, Train Acc: 0.8692 | Val Loss: 0.4240, Val Acc: 0.7920              \n",
    "Epoch 4/6 | Train Loss: 0.2923, Train Acc: 0.8723 | Val Loss: 0.3619, Val Acc: 0.8312              \n",
    "Epoch 5/6 | Train Loss: 0.2845, Train Acc: 0.8753 | Val Loss: 0.3901, Val Acc: 0.8187              \n",
    "Epoch 6/6 | Train Loss: 0.2788, Train Acc: 0.8772 | Val Loss: 0.3979, Val Acc: 0.8223              \n",
    "\n",
    "âœ… All datasets trained successfully!  \n",
    "\n",
    "======================================================================  \n",
    "ðŸ“ˆ Final Validation Accuracy Summary  \n",
    "======================================================================  \n",
    "dataset_2            | Val Acc: 0.8223 | Val Loss: 0.3979  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf37342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698c545f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_url_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m all_labels = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_url_tensor\u001b[49m), \u001b[32m64\u001b[39m):  \u001b[38;5;66;03m# batch size 64\u001b[39;00m\n\u001b[32m      7\u001b[39m         batch_x = test_url_tensor[i:i+\u001b[32m64\u001b[39m].to(device)\n\u001b[32m      8\u001b[39m         batch_y = test_labels_tensor[i:i+\u001b[32m64\u001b[39m].to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_url_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_url_tensor), 64):  # batch size 64\n",
    "        batch_x = test_url_tensor[i:i+64].to(device)\n",
    "        batch_y = test_labels_tensor[i:i+64].to(device)\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        preds = (outputs >= 0.5).long().squeeze(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        \n",
    "        # Conv blocks\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 256, kernel_size=8, padding=4)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(256, 128, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*2, 128)  # 64*2 because bidirectional\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)  # binary output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, embed_dim, seq_len) for Conv1d\n",
    "        \n",
    "        # Conv block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Prepare for LSTM: (batch_size, seq_len, features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, 2*hidden_size)\n",
    "        \n",
    "        # Take the last timestep\n",
    "        x = lstm_out[:, -1, :]  # (batch_size, 128)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae29a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET1 dataset\n",
      "======================================================================\n",
      "[dataset1] Epoch 1/3 | Batch 102/102 | Loss: 0.2697, Acc: 0.8942\n",
      "[dataset1] Epoch 1/3 | Train Loss: 0.4063, Train Acc: 0.8384 | Val Loss: 0.2719, Val Acc: 0.8974\n",
      "[dataset1] Epoch 2/3 | Batch 102/102 | Loss: 0.1222, Acc: 0.9658\n",
      "[dataset1] Epoch 2/3 | Train Loss: 0.1782, Train Acc: 0.9378 | Val Loss: 0.1227, Val Acc: 0.9609\n",
      "[dataset1] Epoch 3/3 | Batch 102/102 | Loss: 0.0869, Acc: 0.9730\n",
      "[dataset1] Epoch 3/3 | Train Loss: 0.1033, Train Acc: 0.9677 | Val Loss: 0.0860, Val Acc: 0.9731\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET2 dataset\n",
      "======================================================================\n",
      "[dataset2] Epoch 1/3 | Batch 47/47 | Loss: 0.0809, Acc: 0.9864\n",
      "[dataset2] Epoch 1/3 | Train Loss: 0.3467, Train Acc: 0.8455 | Val Loss: 0.0146, Val Acc: 0.9977\n",
      "[dataset2] Epoch 2/3 | Batch 47/47 | Loss: 0.0022, Acc: 1.0000\n",
      "[dataset2] Epoch 2/3 | Train Loss: 0.0140, Train Acc: 0.9977 | Val Loss: 0.0102, Val Acc: 0.9981\n",
      "[dataset2] Epoch 3/3 | Batch 47/47 | Loss: 0.0022, Acc: 1.0000\n",
      "[dataset2] Epoch 3/3 | Train Loss: 0.0122, Train Acc: 0.9980 | Val Loss: 0.0098, Val Acc: 0.9984\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET3 dataset\n",
      "======================================================================\n",
      "[dataset3] Epoch 1/3 | Batch 139/139 | Loss: 0.3497, Acc: 0.8435\n",
      "[dataset3] Epoch 1/3 | Train Loss: 0.4518, Train Acc: 0.7863 | Val Loss: 0.3366, Val Acc: 0.8566\n",
      "[dataset3] Epoch 2/3 | Batch 139/139 | Loss: 0.2770, Acc: 0.8722\n",
      "[dataset3] Epoch 2/3 | Train Loss: 0.3166, Train Acc: 0.8633 | Val Loss: 0.2849, Val Acc: 0.8739\n",
      "[dataset3] Epoch 3/3 | Batch 139/139 | Loss: 0.2620, Acc: 0.8855\n",
      "[dataset3] Epoch 3/3 | Train Loss: 0.2841, Train Acc: 0.8752 | Val Loss: 0.2699, Val Acc: 0.8801\n",
      "\n",
      "âœ… All datasets trained successfully!\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ Final Validation Summary (CNN-BiLSTM)\n",
      "======================================================================\n",
      "dataset1             | Val Acc: 0.9731 | Val Loss: 0.0860\n",
      "dataset2             | Val Acc: 0.9984 | Val Loss: 0.0098\n",
      "dataset3             | Val Acc: 0.8801 | Val Loss: 0.2699\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ Training Config\n",
    "# ============================================================\n",
    "num_epochs = 3\n",
    "lr = 0.001\n",
    "batch_print_interval = 1\n",
    "\n",
    "# Dictionary to store all dataset results\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ” Loop Over Each Dataset\n",
    "# ============================================================\n",
    "for dataset_name, loaders in dataloader_dict.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training CNN-BiLSTM on {dataset_name.upper()} dataset\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    train_loader = loaders[\"train_loader\"]\n",
    "    val_loader = loaders[\"val_loader\"]\n",
    "\n",
    "    model = CNN_BiLSTM(vocab_size=len(vocab)).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Track metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device).float().unsqueeze(1)\n",
    "            batch_y = torch.clamp(batch_y, 0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            acc = (preds == batch_y).float().mean().item()\n",
    "\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "            correct_train += (preds == batch_y).sum().item()\n",
    "            total_train += batch_x.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % batch_print_interval == 0:\n",
    "                sys.stdout.write(f\"\\r[{dataset_name}] Epoch {epoch+1}/{num_epochs} | \"\n",
    "                                 f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                                 f\"Loss: {loss.item():.4f}, Acc: {acc:.4f}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        avg_train_loss = train_loss / total_train\n",
    "        train_acc = correct_train / total_train\n",
    "        print()  # newline\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device).float().unsqueeze(1)\n",
    "                batch_y = torch.clamp(batch_y, 0, 1)\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"[{dataset_name}] Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # === Save results for this dataset ===\n",
    "    all_results[dataset_name] = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"final_val_acc\": val_accs[-1],\n",
    "        \"final_val_loss\": val_losses[-1]\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… All datasets trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š Summary of Final Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ Final Validation Summary (CNN-BiLSTM)\")\n",
    "print(\"=\"*70)\n",
    "for name, res in all_results.items():\n",
    "    print(f\"{name:<20} | Val Acc: {res['final_val_acc']:.4f} | Val Loss: {res['final_val_loss']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
