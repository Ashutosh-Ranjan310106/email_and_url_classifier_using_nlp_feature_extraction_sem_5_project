{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2ae588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                url\n",
      "0     2                        https://blog.sockpuppet.us/\n",
      "1     2                  https://blog.apiki.com/seguranca/\n",
      "2     1  http://autoecole-lauriston.com/a/T0RVd056QXlNe...\n",
      "3     1  http://chinpay.site/index.html?hgcFSE@E$Z*DFcG...\n",
      "4     2  http://www.firstfivenebraska.org/blog/article/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:70: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df4['label'][df4['label'] == 1] = 0\n",
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:71: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df4['label'][df4['label'] == 2] = 1\n",
      "c:\\Users\\rrpra\\Documents\\Github\\git_testing\\email_and_url_classifier_using_nlp_feature_extraction_sem_5_project\\three_datasets.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'] = df['label'].apply(lambda x: 1 if x in phishing_labels else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Seed fixed to 42\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "from urllib.parse import urlparse\n",
    "from three_datasets import all_dataset\n",
    "import random\n",
    "tqdm.pandas()\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"âœ… Seed fixed to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d3c3080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "You are using a model of type canine to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5EncoderModel were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,  T5EncoderModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"google/canine-s\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5EncoderModel.from_pretrained(model_name)  \n",
    "model.eval()\n",
    "def get_byt5_embedding(url):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(url, return_tensors=\"pt\", truncation=True, padding=True, max_length=50)\n",
    "    \n",
    "    # Get encoder output (not generate)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # outputs.last_hidden_state: [batch_size, seq_len, hidden_dim]\n",
    "         \n",
    "        # Mean pooling across sequence\n",
    "        emb = outputs.last_hidden_state.mean(dim=1).reshape(-1)\n",
    "    return emb.cpu().numpy()\n",
    "\n",
    "# Example\n",
    "#url = \"http://paypal-login-secure-update.com\"\n",
    "#vector = get_byt5_embedding(url)\n",
    "#print(\"Embedding shape:\", vector.shape)  # e.g. (512,) or (768,) depending on model size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load once globally (so it doesn't reload every call)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"google/canine-s\"  # or \"google/canine-c\" (larger, slower)\n",
    "\n",
    "# Load model + tokenizer only once\n",
    "tokenizer_canine = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_canine = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model_canine.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_canine_embedding(url, max_length=50, pooling=\"mean\"):\n",
    "    \"\"\"\n",
    "    Returns a fixed-length CANINE embedding for a single URL string.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL text to encode.\n",
    "        max_length (int): Maximum length of character sequence (truncate/pad to this).\n",
    "        pooling (str): 'mean' or 'max' for pooling token embeddings into one vector.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1D embedding vector (shape [hidden_dim], e.g. [768]).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Tokenize\n",
    "    encoded = tokenizer_canine(\n",
    "        url,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model_canine(**encoded)\n",
    "    hidden = outputs.last_hidden_state  # [1, seq_len, hidden_dim]\n",
    "    mask = encoded[\"attention_mask\"].unsqueeze(-1).type_as(hidden)  # [1, seq_len, 1]\n",
    "\n",
    "    # Pooling\n",
    "    if pooling == \"mean\":\n",
    "        summed = (hidden * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        pooled = summed / denom  # [1, hidden_dim]\n",
    "    elif pooling == \"max\":\n",
    "        hidden = hidden.masked_fill(mask == 0, -1e9)\n",
    "        pooled = hidden.max(dim=1).values  # [1, hidden_dim]\n",
    "    else:\n",
    "        raise ValueError(\"Pooling must be 'mean' or 'max'\")\n",
    "\n",
    "    return pooled.squeeze(0).cpu().numpy()  # (hidden_dim,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "357bf0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”  Encoding structured URLs for Dataset 1 (Malicious URLs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417732/417732 [01:17<00:00, 5364.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… train: Encoded 417732 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:09<00:00, 5434.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… valid: Encoded 52217 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:09<00:00, 5474.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… test: Encoded 52217 URLs\n",
      "\n",
      "ðŸ”  Encoding structured URLs for Dataset 2 (ndarvind/phiusiil-phishing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188296/188296 [00:23<00:00, 8088.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… train: Encoded 188296 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23537/23537 [00:02<00:00, 7928.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… valid: Encoded 23537 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23537/23537 [00:02<00:00, 8152.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… test: Encoded 23537 URLs\n",
      "\n",
      "ðŸ”  Encoding structured URLs for Dataset 3 (kmack/Phishing_urls)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528097/528097 [01:24<00:00, 6280.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… train: Encoded 528097 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66012/66012 [00:10<00:00, 6308.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… valid: Encoded 66012 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66013/66013 [00:10<00:00, 6489.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âœ… test: Encoded 66013 URLs\n",
      "\n",
      "âœ… All datasets encoded with proper start/end markers and padding!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Character Encoding Setup\n",
    "# ============================================================\n",
    "\n",
    "# Allowed printable ASCII chars\n",
    "ascii_chars = [chr(i) for i in range(32, 127)]\n",
    "\n",
    "# Special control tokens\n",
    "special_tokens = [\n",
    "    '<PAD>', '<UNK>',\n",
    "]\n",
    "\n",
    "# Build vocab and mapping\n",
    "vocab = special_tokens + ascii_chars\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "\n",
    "\n",
    "\n",
    "def encode(text, max_len=100):\n",
    "    indices = torch.full((max_len,), char2idx[\"<PAD>\"], dtype=torch.long)\n",
    "    text = text.lower()[:max_len]\n",
    "    for i, c in enumerate(text):\n",
    "        indices[i] = char2idx.get(c, char2idx[\"<UNK>\"])\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoded_data = {}\n",
    "frac = 1\n",
    "gen = all_dataset()\n",
    "\n",
    "x=0\n",
    "for name, splits in  gen:\n",
    "    encoded_data[name] = {}\n",
    "    print(f\"\\nðŸ”  Encoding structured URLs for {name}...\")\n",
    "\n",
    "    for split_name, df in zip(['train', 'valid', 'test'], splits):\n",
    "        df = df.sample(frac=frac, random_state=42)\n",
    "        df[\"encode\"] = df[\"url\"].progress_apply(encode)\n",
    "        encoded_data[name][split_name] = df\n",
    "        print(f\"  âœ… {split_name}: Encoded {len(df)} URLs\")\n",
    "\n",
    "    x+=1\n",
    "    if x > 2:\n",
    "        next(gen)\n",
    "\n",
    "print(\"\\nâœ… All datasets encoded with proper start/end markers and padding!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eeebb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Creating DataLoaders for Dataset 1 (Malicious URLs)...\n",
      "âœ… DataLoaders ready for Dataset 1 (Malicious URLs) (Train/Val/Test)\n",
      "\n",
      "ðŸ“¦ Creating DataLoaders for Dataset 2 (ndarvind/phiusiil-phishing)...\n",
      "âœ… DataLoaders ready for Dataset 2 (ndarvind/phiusiil-phishing) (Train/Val/Test)\n",
      "\n",
      "ðŸ“¦ Creating DataLoaders for Dataset 3 (kmack/Phishing_urls)...\n",
      "âœ… DataLoaders ready for Dataset 3 (kmack/Phishing_urls) (Train/Val/Test)\n",
      "\n",
      "ðŸš€ All DataLoaders are ready in `dataloader_dict`!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# ============================================================\n",
    "# Convert to TensorDataset and DataLoader\n",
    "# ============================================================\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_dict = {}\n",
    "\n",
    "def make_tensor_dataset(df):\n",
    "    url_tensor = torch.stack(list(df[\"encode\"]))\n",
    "    labels_tensor = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "    return TensorDataset(url_tensor, labels_tensor)\n",
    "\n",
    "for name, splits in encoded_data.items():\n",
    "    dataloader_dict[name] = {}\n",
    "    print(f\"\\nðŸ“¦ Creating DataLoaders for {name}...\")\n",
    "    \n",
    "    train_set = make_tensor_dataset(splits[\"train\"])\n",
    "    val_set = make_tensor_dataset(splits[\"valid\"])\n",
    "    test_set = make_tensor_dataset(splits[\"test\"])\n",
    "    \n",
    "    dataloader_dict[name][\"train_loader\"] = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    dataloader_dict[name][\"val_loader\"] = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    dataloader_dict[name][\"test_loader\"] = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(f\"âœ… DataLoaders ready for {name} (Train/Val/Test)\")\n",
    "\n",
    "print(\"\\nðŸš€ All DataLoaders are ready in `dataloader_dict`!\")\n",
    "\n",
    "# Example Access:\n",
    "# dataloader_dict[\"dataset1\"][\"train_loader\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea9ea480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# =====================================================\n",
    "# ðŸ”¹ TinyByT5 Encoder (Reduced depth for short sequences)\n",
    "# =====================================================\n",
    "class TinyByT5Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=256,\n",
    "                 d_model=128,\n",
    "                 num_layers=2,\n",
    "                 num_heads=2,\n",
    "                 dim_ff=256,\n",
    "                 max_len=100,\n",
    "                 n_out=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # ðŸ”¹ Byte embedding layer (0â€“255)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # ðŸ”¹ Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # ðŸ”¹ Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # ðŸ”¹ Final normalization\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.projection = nn.Linear(in_features=d_model, out_features=n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) â€” byte indices [0â€“255]\n",
    "        \"\"\"\n",
    "        #batch_size, seq_len = x.size()\n",
    "        #device = x.device\n",
    "\n",
    "        #positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        #x = self.embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        #x = self.encoder(x)\n",
    "        x = self.embedding(x)\n",
    "        x = self.projection(self.final_norm(x))\n",
    "\n",
    "        return x  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53464d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        w = x.mean(dim=2)                          # Global Average Pooling -> (B, C)\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = self.sigmoid(self.fc2(w))\n",
    "        w = w.unsqueeze(2)                         # (B, C, 1)\n",
    "        return x * w                               # scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cc210f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Residual Depthwise-Separable Multi-Kernel Block\n",
    "class ResidualConvBlockDW(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_sizes=[3, 5, 7], reduction=16):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList()\n",
    "\n",
    "        mid_ch = max(in_ch // 2, 8)  # reduce dimension before heavy convs\n",
    "\n",
    "        for k in kernel_sizes:\n",
    "            branch = nn.Sequential(\n",
    "                # (B) Reduce channels first\n",
    "                nn.Conv1d(in_ch, mid_ch, kernel_size=1, bias=False),\n",
    "                #nn.BatchNorm1d(mid_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                # (A) Depthwise conv\n",
    "                nn.Conv1d(mid_ch, mid_ch, kernel_size=k, padding=k // 2, groups=mid_ch, bias=False),\n",
    "                #nn.BatchNorm1d(mid_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                # Pointwise to expand to out_ch\n",
    "                nn.Conv1d(mid_ch, out_ch, kernel_size=1, bias=False),\n",
    "                #nn.BatchNorm1d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.branches.append(branch)\n",
    "\n",
    "        # Combine all kernel branches\n",
    "        self.merge_conv = nn.Conv1d(out_ch * len(kernel_sizes), out_ch, kernel_size=1, bias=False)\n",
    "        #self.merge_bn = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        self.se = SEBlock(out_ch, reduction)\n",
    "        self.shortcut = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Parallel multi-kernel branches\n",
    "        out = [branch(x) for branch in self.branches]\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        out = self.merge_conv(out)\n",
    "        #out = self.merge_bn(out)\n",
    "        out = self.se(out)\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6ac0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class URLBinaryCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, maxlen=100):\n",
    "        super(URLBinaryCNN, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "        self.transformer = TinyByT5Encoder(\n",
    "            vocab_size=vocab_size,\n",
    "            max_len=maxlen,\n",
    "            d_model=512,\n",
    "            n_out=embed_dim\n",
    "        )\n",
    "        # ðŸ”¹ 1st Conv block\n",
    "        self.conv_layer1 = ResidualConvBlockDW(embed_dim, 128)\n",
    "        self.conv_layer2 = ResidualConvBlockDW(64, 32)\n",
    "\n",
    "\n",
    "         # ðŸ”¹ 2st Conv block\n",
    "        self.conv2_3x3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.convreduce2_1x1 = nn.Conv1d(in_channels=128, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.conv2_5x5 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.conv2_1x1 = nn.Conv1d(in_channels=64*2, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=[64, maxlen])\n",
    "        \n",
    "\n",
    "         # ðŸ”¹ 3st Conv block\n",
    "        self.conv3_3x3 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.convreduce3_1x1 = nn.Conv1d(in_channels=64, out_channels=1, kernel_size=1, padding=0)\n",
    "        self.conv3_5x5 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.conv3_7 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.conv3_1x1 = nn.Conv1d(in_channels=32*3, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.layer_norm3 = nn.LayerNorm(normalized_shape=[32, maxlen])\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, \n",
    "                            batch_first=True)\n",
    "        self.bilstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "        #self.aap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # ðŸ”¹ Fully connected layers\n",
    "        self.layer_norm = nn.LayerNorm(128 * 2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128*2, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "\n",
    "\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len)\n",
    "        x = self.transformer(x)           # (B, L, E)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)            # (B, E, L)\n",
    "\n",
    "\n",
    "\n",
    "        # ðŸ”¹ Block 1\n",
    "        #x3 = F.relu(self.conv1_3x3(x))\n",
    "\n",
    "        #xr = F.relu(self.convreduce1_1x1(x))\n",
    "        #x5 = F.relu(self.conv1_5x5(x))\n",
    "        #x7 = F.relu(self.conv1_7(x))\n",
    "\n",
    "        #x = torch.cat([x3, x5, x7], dim=1)\n",
    "        #print(x.shape)\n",
    "        #x = F.relu(self.conv1_1x1(x))\n",
    "\n",
    "        #x = self.layer_norm1(x)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # ðŸ”¹ Block 2\n",
    "        x3 = F.relu(self.conv2_3x3(x))\n",
    "        xr = F.relu(self.convreduce2_1x1(x))\n",
    "        x5 = F.relu(self.conv2_5x5(xr))\n",
    "        x = torch.cat([x3, x5], dim=1)\n",
    "        x = F.relu(self.conv2_1x1(x))\n",
    "        x = self.layer_norm2(x)\n",
    "        '''\n",
    "        # ðŸ”¹ Block 3\n",
    "        #x = F.relu(self.conv3_3x3(x))\n",
    "        #xr = F.relu(self.convreduce3_1x1(x))\n",
    "        #x5 = F.relu(self.conv3_5x5(x))\n",
    "        #x7 = F.relu(self.conv3_7(x))\n",
    "        #x = torch.cat([x3, x5, x7], dim=1)\n",
    "        #x = F.relu(self.conv3_1x1(x))\n",
    "        #x = self.layer_norm3(x)\n",
    "        x = self.conv_layer1(x)\n",
    "        #x = self.conv_layer2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x[:, -1, :]\n",
    "        # ðŸ”¹ Global Average Pooling + FC\n",
    "        #x = self.aap(x)                   # (batch, channels, 1)\n",
    "        x = self.flatten(x)               # (batch, channels)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc_layers(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"Return deep features before final FC layers.\"\"\"\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.bilstm(x)\n",
    "        x = x[:, -1, :]          # shape: [batch, 32]\n",
    "        x = self.flatten(x) \n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64290975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 criterion, \n",
    "                 transformer_optimizer=None,\n",
    "                 main_optimizer = None, \n",
    "                 scheduler_t=None,\n",
    "                 scheduler_c=None,\n",
    "                 train_loader=None, \n",
    "                 val_loader=None,   \n",
    "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        \n",
    "        \"\"\"\n",
    "        optimizer_groups: dict with keys like {\"transformer\": optimizer1, \"cnn\": optimizer2}\n",
    "        schedulers: dict with keys matching optimizer_groups (optional)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.transformer_params = []\n",
    "        self.cnn_params = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if name.startswith(\"transformer.\"):\n",
    "                self.transformer_params.append(param)\n",
    "            else:\n",
    "                self.cnn_params.append(param)\n",
    "\n",
    "\n",
    "\n",
    "        self.transformer_optimizer = optim.Adam(self.transformer_params, lr=1e-4) if transformer_optimizer is None  else transformer_optimizer\n",
    "        self.main_optimizer = optim.Adam(self.cnn_params, lr=1e-3) if main_optimizer is None else main_optimizer\n",
    "        self.scheduler_t = optim.lr_scheduler.ReduceLROnPlateau(self.transformer_optimizer, mode='min', factor=0.5, patience=2) if scheduler_t is None  else scheduler_t\n",
    "        self.scheduler_c = optim.lr_scheduler.ReduceLROnPlateau(self.main_optimizer, mode='min', factor=0.5, patience=2) if scheduler_c is None  else scheduler_c\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_losses, self.val_losses = [], []\n",
    "        self.train_accs, self.val_accs = [], []\n",
    "\n",
    "\n",
    "    def freeze_module(self, module):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_module(self, module):\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def train(self, epochs_list=[3,3,4], early_stopping=True, frac=1.0, val_frac=1.0, alt_cycle = 2,  log=0):\n",
    "        for phase, epochs in enumerate(epochs_list):\n",
    "            for epoch in range(epochs):\n",
    "                phase=3\n",
    "                if phase == 0:\n",
    "                    # ðŸ§  Train Transformer â€” freeze CNN/LSTM layers\n",
    "                    self.unfreeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        if name != \"transformer\":\n",
    "                            self.freeze_module(module)\n",
    "                        else:\n",
    "                            self.unfreeze_module(module)\n",
    "                    active_optims = [self.transformer_optimizer]\n",
    "                    active_scheds = [self.scheduler_t]\n",
    "                    phase_name = \"Transformer\"\n",
    "                elif phase == 1:\n",
    "                    # ðŸŽ¯ Train CNN/LSTM/FC â€” freeze Transformer\n",
    "                    self.freeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        if name == \"transformer\":\n",
    "                            self.freeze_module(module)\n",
    "                        else:\n",
    "                            self.unfreeze_module(module)\n",
    "                    active_optims = [self.main_optimizer]\n",
    "                    active_scheds = [self.scheduler_c]\n",
    "                    phase_name = \"CNN\"\n",
    "                else:\n",
    "                    self.freeze_module(self.model.transformer)\n",
    "                    for name, module in self.model.named_children():\n",
    "                        self.unfreeze_module(module)\n",
    "                    active_optims = [self.transformer_optimizer, self.main_optimizer]\n",
    "                    active_scheds = [self.scheduler_t, self.scheduler_c]\n",
    "                    phase_name = \"CNN + transformer\"\n",
    "\n",
    "\n",
    "\n",
    "                self.model.train()\n",
    "                train_loss, correct_train, total_train = 0, 0, 0\n",
    "                max_batches = int(len(self.train_loader) * frac)\n",
    "                \n",
    "                for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
    "                    if batch_idx >= max_batches:\n",
    "                        break\n",
    "                    \n",
    "                    batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True).float().unsqueeze(1)\n",
    "                    \n",
    "                    for opt in active_optims:\n",
    "                        opt.zero_grad()\n",
    "\n",
    "                    outputs = self.model(batch_x)\n",
    "                    loss = self.criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "\n",
    "                    for opt in active_optims:\n",
    "                        opt.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # === Metrics ===\n",
    "                    batch_loss = loss.item()\n",
    "                    preds = (outputs >= 0.5).float()\n",
    "                    batch_acc = (preds == batch_y).float().mean().item()\n",
    "\n",
    "                    if log >= 1 and (batch_idx + 1) % (20/log) == 0:\n",
    "                        print(f\"\\rEpoch {epoch+1}/{epochs}: Training {phase_name} | \"\n",
    "                            f\"Batch {batch_idx+1}/{max_batches} | \"\n",
    "                            f\"Loss: {batch_loss:.4f}, Acc: {batch_acc:.4f}\", end='')\n",
    "\n",
    "\n",
    "                if log >= 2:\n",
    "                    max_batches = max(int(len(self.train_loader) * frac), 1)\n",
    "                    print(f'\\r total training batch size {max_batches}'.ljust(100), end='')\n",
    "                    with torch.no_grad():\n",
    "                        for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
    "                            if batch_idx >= max_batches:\n",
    "                                break\n",
    "                            \n",
    "                            batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True).float().unsqueeze(1)\n",
    "\n",
    "                            outputs = self.model(batch_x)\n",
    "                            loss = self.criterion(outputs, batch_y)\n",
    "\n",
    "                            batch_loss = loss.item()\n",
    "                            preds = (outputs >= 0.5).float()\n",
    "                            batch_acc = (preds == batch_y).float().mean().item()\n",
    "\n",
    "                            train_loss += batch_loss * batch_x.size(0)\n",
    "                            correct_train += (preds == batch_y).sum().item()\n",
    "                            total_train += batch_x.size(0)\n",
    "\n",
    "                        avg_train_loss = train_loss / total_train\n",
    "                        train_acc = correct_train / total_train\n",
    "                        self.train_losses.append(avg_train_loss)\n",
    "                        self.train_accs.append(train_acc)\n",
    "\n",
    "\n",
    "                    # === Validation ===\n",
    "                    if self.val_loader is not None:\n",
    "                        avg_val_loss, val_acc = self.evaluate(val_frac)\n",
    "                        self.val_losses.append(avg_val_loss)\n",
    "                        self.val_accs.append(val_acc)\n",
    "\n",
    "                        print(f\"\\rEpoch {epoch+1}/{epochs} Training {phase_name}| \"\n",
    "                            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                            f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "                    else:\n",
    "                        print(f\"\\rEpoch {epoch+1}/{epochs} Training {phase_name}| \"\n",
    "                            f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "                for sched in active_scheds:\n",
    "                    sched.step(avg_val_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, frac=1.0):\n",
    "        self.model.eval()\n",
    "        val_loss, correct_val, total_val = 0, 0, 0\n",
    "        max_batches = max(int(len(self.val_loader) * frac), 1)\n",
    "        print(f'\\r total validation batch size {max_batches}'.ljust(100), end='')\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(self.val_loader):\n",
    "                if batch_idx >= max_batches:\n",
    "                    break\n",
    "                batch_x, batch_y = batch_x.to(self.device, non_blocking=True), batch_y.to(self.device, non_blocking=True).float().unsqueeze(1)\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                avg_batch_loss = loss.item()\n",
    "                val_loss += avg_batch_loss * batch_x.size(0)\n",
    "                preds = (outputs >= 0.5).float()\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_acc = correct_val / total_val\n",
    "        return avg_val_loss, val_acc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "14896532",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Training model on DATASET 1 (MALICIOUS URLS) dataset\n",
      "======================================================================\n",
      "__________ðŸ§© Using 100% of training data___________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0912, Train Acc: 0.9731 | Val Loss: 0.0886, Val Acc: 0.9727\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0705, Train Acc: 0.9776 | Val Loss: 0.0700, Val Acc: 0.9768\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0623, Train Acc: 0.9810 | Val Loss: 0.0614, Val Acc: 0.9805\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training model on DATASET 2 (NDARVIND/PHIUSIIL-PHISHING) dataset\n",
      "======================================================================\n",
      "__________ðŸ§© Using 100% of training data___________\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0161, Train Acc: 0.9976 | Val Loss: 0.0157, Val Acc: 0.9974\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0149, Train Acc: 0.9979 | Val Loss: 0.0139, Val Acc: 0.9977\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.0147, Train Acc: 0.9980 | Val Loss: 0.0157, Val Acc: 0.9977\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training model on DATASET 3 (KMACK/PHISHING_URLS) dataset\n",
      "======================================================================\n",
      "__________ðŸ§© Using 100% of training data___________\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.2989, Train Acc: 0.8804 | Val Loss: 0.2861, Val Acc: 0.8802\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.2346, Train Acc: 0.9077 | Val Loss: 0.2349, Val Acc: 0.9053\n",
      "Epoch 1/1 Training CNN + transformer| Train Loss: 0.2327, Train Acc: 0.9086 | Val Loss: 0.2300, Val Acc: 0.9065\n",
      "\n",
      "âœ… All datasets trained successfully!\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ Final Validation Accuracy Summary\n",
      "======================================================================\n",
      "Dataset 1 (Malicious URLs) | Val Acc: 0.9805 | Val Loss: 0.0614\n",
      "Dataset 2 (ndarvind/phiusiil-phishing) | Val Acc: 0.9977 | Val Loss: 0.0157\n",
      "Dataset 3 (kmack/Phishing_urls) | Val Acc: 0.9065 | Val Loss: 0.2300\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ Training Config\n",
    "# ============================================================\n",
    "num_epochs = [1,1,1]\n",
    "lr = 0.01\n",
    "\n",
    "# Store all dataset metrics\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ” Training Loop for Each Dataset\n",
    "# ============================================================\n",
    "nn_model = {}\n",
    "for dataset_name, loaders in dataloader_dict.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training model on {dataset_name.upper()} dataset\")\n",
    "    print(\"=\"*70)\n",
    "    for frac in  [1.0]:\n",
    "        print(f\"ðŸ§© Using {frac*100:.0f}% of training data\".center(50, '_'))\n",
    "        train_loader = loaders[\"train_loader\"]\n",
    "        val_loader = loaders[\"val_loader\"]\n",
    "\n",
    "        # Initialize model, loss, optimizer\n",
    "        nn_model[dataset_name] = URLBinaryCNN(vocab_size=len(vocab)).to(device)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(nn_model[dataset_name].parameters(), lr=lr, weight_decay=lr/10)\n",
    "        trainer = Train(nn_model[dataset_name], criterion, train_loader=train_loader, val_loader=val_loader)\n",
    "        # Lists to track performance\n",
    "        trainer.train(num_epochs,lr,frac=frac,val_frac=frac, log=2)\n",
    "\n",
    "    \n",
    "\n",
    "    # Store all results for this dataset\n",
    "    all_results[dataset_name] = {\n",
    "        \"train_losses\": trainer.train_losses,\n",
    "        \"val_losses\": trainer.val_losses,\n",
    "        \"train_accs\": trainer.train_accs,\n",
    "        \"val_accs\": trainer.val_accs,\n",
    "        \"final_val_acc\": trainer.val_accs[-1],\n",
    "        \"final_val_loss\": trainer.val_losses[-1]\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… All datasets trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š Summary of All Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ Final Validation Accuracy Summary\")\n",
    "print(\"=\"*70)\n",
    "for name, res in all_results.items():\n",
    "    print(f\"{name:<20} | Val Acc: {res['final_val_acc']:.4f} | Val Loss: {res['final_val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4785fe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"url_cnn_lstm.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64236ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbost model + cnn based dl model\n",
    "def extract_features(self, x):\n",
    "        \"\"\"Return deep features before final FC layers.\"\"\"\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x3 = F.relu(self.conv1_3x3(x))\n",
    "        x5 = F.relu(self.conv1_5x5(x))\n",
    "        x7 = F.relu(self.conv1_7(x))\n",
    "        x = torch.cat([x3, x5, x7], dim=1)\n",
    "        x = F.relu(self.conv1_1x1(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]          # shape: [batch, 32]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f2ad9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ” Extracting Train Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16504/16504 [04:56<00:00, 55.76batch/s]\n",
      "ðŸ” Extracting Val Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2063/2063 [00:39<00:00, 52.13batch/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "features, labels = [], []\n",
    "\n",
    "# ðŸ”¹ Extract CNN features for training set\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(train_loader, desc=\"ðŸ” Extracting Train Features\", unit=\"batch\"):\n",
    "        x_batch = x_batch.to(device, non_blocking=True)\n",
    "        feats = nn_model[dataset_name].extract_features(x_batch)\n",
    "        features.append(feats.cpu().numpy())\n",
    "        labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "X_train = np.concatenate(features, axis=0)\n",
    "y_train = np.concatenate(labels, axis=0)\n",
    "\n",
    "# Free memory before val extraction\n",
    "del features, labels, x_batch, y_batch, feats\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ðŸ”¹ Extract CNN features for validation set\n",
    "features, labels = [], []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm(val_loader, desc=\"ðŸ” Extracting Val Features\", unit=\"batch\"):\n",
    "        x_batch = x_batch.to(device, non_blocking=True)\n",
    "        feats = nn_model[dataset_name].extract_features(x_batch)\n",
    "        features.append(feats.cpu().numpy())\n",
    "        labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "X_val = np.concatenate(features, axis=0)\n",
    "y_val = np.concatenate(labels, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de5dd127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.68590\n",
      "[1]\tvalidation_0-logloss:0.67879\n",
      "[2]\tvalidation_0-logloss:0.67182\n",
      "[3]\tvalidation_0-logloss:0.66499\n",
      "[4]\tvalidation_0-logloss:0.65829\n",
      "[5]\tvalidation_0-logloss:0.65172\n",
      "[6]\tvalidation_0-logloss:0.64528\n",
      "[7]\tvalidation_0-logloss:0.63895\n",
      "[8]\tvalidation_0-logloss:0.63275\n",
      "[9]\tvalidation_0-logloss:0.62666\n",
      "[10]\tvalidation_0-logloss:0.62068\n",
      "[11]\tvalidation_0-logloss:0.61481\n",
      "[12]\tvalidation_0-logloss:0.60905\n",
      "[13]\tvalidation_0-logloss:0.60339\n",
      "[14]\tvalidation_0-logloss:0.59784\n",
      "[15]\tvalidation_0-logloss:0.59238\n",
      "[16]\tvalidation_0-logloss:0.58701\n",
      "[17]\tvalidation_0-logloss:0.58174\n",
      "[18]\tvalidation_0-logloss:0.57656\n",
      "[19]\tvalidation_0-logloss:0.57148\n",
      "[20]\tvalidation_0-logloss:0.56648\n",
      "[21]\tvalidation_0-logloss:0.56156\n",
      "[22]\tvalidation_0-logloss:0.55673\n",
      "[23]\tvalidation_0-logloss:0.55198\n",
      "[24]\tvalidation_0-logloss:0.54731\n",
      "[25]\tvalidation_0-logloss:0.54272\n",
      "[26]\tvalidation_0-logloss:0.53821\n",
      "[27]\tvalidation_0-logloss:0.53377\n",
      "[28]\tvalidation_0-logloss:0.52941\n",
      "[29]\tvalidation_0-logloss:0.52512\n",
      "[30]\tvalidation_0-logloss:0.52090\n",
      "[31]\tvalidation_0-logloss:0.51674\n",
      "[32]\tvalidation_0-logloss:0.51265\n",
      "[33]\tvalidation_0-logloss:0.50863\n",
      "[34]\tvalidation_0-logloss:0.50468\n",
      "[35]\tvalidation_0-logloss:0.50079\n",
      "[36]\tvalidation_0-logloss:0.49696\n",
      "[37]\tvalidation_0-logloss:0.49318\n",
      "[38]\tvalidation_0-logloss:0.48948\n",
      "[39]\tvalidation_0-logloss:0.48582\n",
      "[40]\tvalidation_0-logloss:0.48223\n",
      "[41]\tvalidation_0-logloss:0.47868\n",
      "[42]\tvalidation_0-logloss:0.47520\n",
      "[43]\tvalidation_0-logloss:0.47177\n",
      "[44]\tvalidation_0-logloss:0.46839\n",
      "[45]\tvalidation_0-logloss:0.46507\n",
      "[46]\tvalidation_0-logloss:0.46179\n",
      "[47]\tvalidation_0-logloss:0.45856\n",
      "[48]\tvalidation_0-logloss:0.45539\n",
      "[49]\tvalidation_0-logloss:0.45226\n",
      "[50]\tvalidation_0-logloss:0.44918\n",
      "[51]\tvalidation_0-logloss:0.44614\n",
      "[52]\tvalidation_0-logloss:0.44316\n",
      "[53]\tvalidation_0-logloss:0.44021\n",
      "[54]\tvalidation_0-logloss:0.43730\n",
      "[55]\tvalidation_0-logloss:0.43444\n",
      "[56]\tvalidation_0-logloss:0.43163\n",
      "[57]\tvalidation_0-logloss:0.42885\n",
      "[58]\tvalidation_0-logloss:0.42611\n",
      "[59]\tvalidation_0-logloss:0.42342\n",
      "[60]\tvalidation_0-logloss:0.42076\n",
      "[61]\tvalidation_0-logloss:0.41814\n",
      "[62]\tvalidation_0-logloss:0.41556\n",
      "[63]\tvalidation_0-logloss:0.41302\n",
      "[64]\tvalidation_0-logloss:0.41052\n",
      "[65]\tvalidation_0-logloss:0.40805\n",
      "[66]\tvalidation_0-logloss:0.40562\n",
      "[67]\tvalidation_0-logloss:0.40322\n",
      "[68]\tvalidation_0-logloss:0.40085\n",
      "[69]\tvalidation_0-logloss:0.39853\n",
      "[70]\tvalidation_0-logloss:0.39623\n",
      "[71]\tvalidation_0-logloss:0.39396\n",
      "[72]\tvalidation_0-logloss:0.39173\n",
      "[73]\tvalidation_0-logloss:0.38952\n",
      "[74]\tvalidation_0-logloss:0.38736\n",
      "[75]\tvalidation_0-logloss:0.38521\n",
      "[76]\tvalidation_0-logloss:0.38309\n",
      "[77]\tvalidation_0-logloss:0.38101\n",
      "[78]\tvalidation_0-logloss:0.37895\n",
      "[79]\tvalidation_0-logloss:0.37692\n",
      "[80]\tvalidation_0-logloss:0.37492\n",
      "[81]\tvalidation_0-logloss:0.37295\n",
      "[82]\tvalidation_0-logloss:0.37100\n",
      "[83]\tvalidation_0-logloss:0.36908\n",
      "[84]\tvalidation_0-logloss:0.36719\n",
      "[85]\tvalidation_0-logloss:0.36532\n",
      "[86]\tvalidation_0-logloss:0.36348\n",
      "[87]\tvalidation_0-logloss:0.36167\n",
      "[88]\tvalidation_0-logloss:0.35988\n",
      "[89]\tvalidation_0-logloss:0.35811\n",
      "[90]\tvalidation_0-logloss:0.35636\n",
      "[91]\tvalidation_0-logloss:0.35464\n",
      "[92]\tvalidation_0-logloss:0.35295\n",
      "[93]\tvalidation_0-logloss:0.35127\n",
      "[94]\tvalidation_0-logloss:0.34962\n",
      "[95]\tvalidation_0-logloss:0.34800\n",
      "[96]\tvalidation_0-logloss:0.34640\n",
      "[97]\tvalidation_0-logloss:0.34482\n",
      "[98]\tvalidation_0-logloss:0.34325\n",
      "[99]\tvalidation_0-logloss:0.34171\n",
      "[100]\tvalidation_0-logloss:0.34019\n",
      "[101]\tvalidation_0-logloss:0.33869\n",
      "[102]\tvalidation_0-logloss:0.33721\n",
      "[103]\tvalidation_0-logloss:0.33574\n",
      "[104]\tvalidation_0-logloss:0.33430\n",
      "[105]\tvalidation_0-logloss:0.33289\n",
      "[106]\tvalidation_0-logloss:0.33148\n",
      "[107]\tvalidation_0-logloss:0.33009\n",
      "[108]\tvalidation_0-logloss:0.32873\n",
      "[109]\tvalidation_0-logloss:0.32737\n",
      "[110]\tvalidation_0-logloss:0.32604\n",
      "[111]\tvalidation_0-logloss:0.32473\n",
      "[112]\tvalidation_0-logloss:0.32343\n",
      "[113]\tvalidation_0-logloss:0.32214\n",
      "[114]\tvalidation_0-logloss:0.32088\n",
      "[115]\tvalidation_0-logloss:0.31963\n",
      "[116]\tvalidation_0-logloss:0.31839\n",
      "[117]\tvalidation_0-logloss:0.31718\n",
      "[118]\tvalidation_0-logloss:0.31598\n",
      "[119]\tvalidation_0-logloss:0.31479\n",
      "[120]\tvalidation_0-logloss:0.31362\n",
      "[121]\tvalidation_0-logloss:0.31247\n",
      "[122]\tvalidation_0-logloss:0.31133\n",
      "[123]\tvalidation_0-logloss:0.31021\n",
      "[124]\tvalidation_0-logloss:0.30910\n",
      "[125]\tvalidation_0-logloss:0.30800\n",
      "[126]\tvalidation_0-logloss:0.30691\n",
      "[127]\tvalidation_0-logloss:0.30584\n",
      "[128]\tvalidation_0-logloss:0.30479\n",
      "[129]\tvalidation_0-logloss:0.30374\n",
      "[130]\tvalidation_0-logloss:0.30272\n",
      "[131]\tvalidation_0-logloss:0.30170\n",
      "[132]\tvalidation_0-logloss:0.30070\n",
      "[133]\tvalidation_0-logloss:0.29971\n",
      "[134]\tvalidation_0-logloss:0.29873\n",
      "[135]\tvalidation_0-logloss:0.29777\n",
      "[136]\tvalidation_0-logloss:0.29682\n",
      "[137]\tvalidation_0-logloss:0.29588\n",
      "[138]\tvalidation_0-logloss:0.29495\n",
      "[139]\tvalidation_0-logloss:0.29403\n",
      "[140]\tvalidation_0-logloss:0.29313\n",
      "[141]\tvalidation_0-logloss:0.29223\n",
      "[142]\tvalidation_0-logloss:0.29135\n",
      "[143]\tvalidation_0-logloss:0.29048\n",
      "[144]\tvalidation_0-logloss:0.28962\n",
      "[145]\tvalidation_0-logloss:0.28877\n",
      "[146]\tvalidation_0-logloss:0.28794\n",
      "[147]\tvalidation_0-logloss:0.28711\n",
      "[148]\tvalidation_0-logloss:0.28629\n",
      "[149]\tvalidation_0-logloss:0.28549\n",
      "[150]\tvalidation_0-logloss:0.28469\n",
      "[151]\tvalidation_0-logloss:0.28390\n",
      "[152]\tvalidation_0-logloss:0.28312\n",
      "[153]\tvalidation_0-logloss:0.28235\n",
      "[154]\tvalidation_0-logloss:0.28159\n",
      "[155]\tvalidation_0-logloss:0.28084\n",
      "[156]\tvalidation_0-logloss:0.28010\n",
      "[157]\tvalidation_0-logloss:0.27936\n",
      "[158]\tvalidation_0-logloss:0.27864\n",
      "[159]\tvalidation_0-logloss:0.27792\n",
      "[160]\tvalidation_0-logloss:0.27722\n",
      "[161]\tvalidation_0-logloss:0.27652\n",
      "[162]\tvalidation_0-logloss:0.27583\n",
      "[163]\tvalidation_0-logloss:0.27515\n",
      "[164]\tvalidation_0-logloss:0.27448\n",
      "[165]\tvalidation_0-logloss:0.27382\n",
      "[166]\tvalidation_0-logloss:0.27316\n",
      "[167]\tvalidation_0-logloss:0.27252\n",
      "[168]\tvalidation_0-logloss:0.27188\n",
      "[169]\tvalidation_0-logloss:0.27125\n",
      "[170]\tvalidation_0-logloss:0.27063\n",
      "[171]\tvalidation_0-logloss:0.27001\n",
      "[172]\tvalidation_0-logloss:0.26941\n",
      "[173]\tvalidation_0-logloss:0.26881\n",
      "[174]\tvalidation_0-logloss:0.26822\n",
      "[175]\tvalidation_0-logloss:0.26763\n",
      "[176]\tvalidation_0-logloss:0.26705\n",
      "[177]\tvalidation_0-logloss:0.26648\n",
      "[178]\tvalidation_0-logloss:0.26592\n",
      "[179]\tvalidation_0-logloss:0.26536\n",
      "[180]\tvalidation_0-logloss:0.26481\n",
      "[181]\tvalidation_0-logloss:0.26426\n",
      "[182]\tvalidation_0-logloss:0.26372\n",
      "[183]\tvalidation_0-logloss:0.26319\n",
      "[184]\tvalidation_0-logloss:0.26266\n",
      "[185]\tvalidation_0-logloss:0.26214\n",
      "[186]\tvalidation_0-logloss:0.26163\n",
      "[187]\tvalidation_0-logloss:0.26113\n",
      "[188]\tvalidation_0-logloss:0.26063\n",
      "[189]\tvalidation_0-logloss:0.26014\n",
      "[190]\tvalidation_0-logloss:0.25965\n",
      "[191]\tvalidation_0-logloss:0.25917\n",
      "[192]\tvalidation_0-logloss:0.25869\n",
      "[193]\tvalidation_0-logloss:0.25822\n",
      "[194]\tvalidation_0-logloss:0.25776\n",
      "[195]\tvalidation_0-logloss:0.25730\n",
      "[196]\tvalidation_0-logloss:0.25685\n",
      "[197]\tvalidation_0-logloss:0.25641\n",
      "[198]\tvalidation_0-logloss:0.25598\n",
      "[199]\tvalidation_0-logloss:0.25555\n",
      "[200]\tvalidation_0-logloss:0.25513\n",
      "[201]\tvalidation_0-logloss:0.25471\n",
      "[202]\tvalidation_0-logloss:0.25430\n",
      "[203]\tvalidation_0-logloss:0.25388\n",
      "[204]\tvalidation_0-logloss:0.25347\n",
      "[205]\tvalidation_0-logloss:0.25307\n",
      "[206]\tvalidation_0-logloss:0.25267\n",
      "[207]\tvalidation_0-logloss:0.25228\n",
      "[208]\tvalidation_0-logloss:0.25190\n",
      "[209]\tvalidation_0-logloss:0.25152\n",
      "[210]\tvalidation_0-logloss:0.25114\n",
      "[211]\tvalidation_0-logloss:0.25076\n",
      "[212]\tvalidation_0-logloss:0.25039\n",
      "[213]\tvalidation_0-logloss:0.25002\n",
      "[214]\tvalidation_0-logloss:0.24966\n",
      "[215]\tvalidation_0-logloss:0.24931\n",
      "[216]\tvalidation_0-logloss:0.24895\n",
      "[217]\tvalidation_0-logloss:0.24860\n",
      "[218]\tvalidation_0-logloss:0.24826\n",
      "[219]\tvalidation_0-logloss:0.24792\n",
      "[220]\tvalidation_0-logloss:0.24759\n",
      "[221]\tvalidation_0-logloss:0.24726\n",
      "[222]\tvalidation_0-logloss:0.24694\n",
      "[223]\tvalidation_0-logloss:0.24662\n",
      "[224]\tvalidation_0-logloss:0.24630\n",
      "[225]\tvalidation_0-logloss:0.24598\n",
      "[226]\tvalidation_0-logloss:0.24568\n",
      "[227]\tvalidation_0-logloss:0.24537\n",
      "[228]\tvalidation_0-logloss:0.24507\n",
      "[229]\tvalidation_0-logloss:0.24477\n",
      "[230]\tvalidation_0-logloss:0.24448\n",
      "[231]\tvalidation_0-logloss:0.24418\n",
      "[232]\tvalidation_0-logloss:0.24389\n",
      "[233]\tvalidation_0-logloss:0.24361\n",
      "[234]\tvalidation_0-logloss:0.24333\n",
      "[235]\tvalidation_0-logloss:0.24305\n",
      "[236]\tvalidation_0-logloss:0.24278\n",
      "[237]\tvalidation_0-logloss:0.24251\n",
      "[238]\tvalidation_0-logloss:0.24224\n",
      "[239]\tvalidation_0-logloss:0.24197\n",
      "[240]\tvalidation_0-logloss:0.24171\n",
      "[241]\tvalidation_0-logloss:0.24145\n",
      "[242]\tvalidation_0-logloss:0.24120\n",
      "[243]\tvalidation_0-logloss:0.24095\n",
      "[244]\tvalidation_0-logloss:0.24070\n",
      "[245]\tvalidation_0-logloss:0.24045\n",
      "[246]\tvalidation_0-logloss:0.24020\n",
      "[247]\tvalidation_0-logloss:0.23996\n",
      "[248]\tvalidation_0-logloss:0.23973\n",
      "[249]\tvalidation_0-logloss:0.23950\n",
      "[250]\tvalidation_0-logloss:0.23927\n",
      "[251]\tvalidation_0-logloss:0.23904\n",
      "[252]\tvalidation_0-logloss:0.23882\n",
      "[253]\tvalidation_0-logloss:0.23859\n",
      "[254]\tvalidation_0-logloss:0.23838\n",
      "[255]\tvalidation_0-logloss:0.23816\n",
      "[256]\tvalidation_0-logloss:0.23795\n",
      "[257]\tvalidation_0-logloss:0.23773\n",
      "[258]\tvalidation_0-logloss:0.23752\n",
      "[259]\tvalidation_0-logloss:0.23732\n",
      "[260]\tvalidation_0-logloss:0.23711\n",
      "[261]\tvalidation_0-logloss:0.23691\n",
      "[262]\tvalidation_0-logloss:0.23671\n",
      "[263]\tvalidation_0-logloss:0.23652\n",
      "[264]\tvalidation_0-logloss:0.23633\n",
      "[265]\tvalidation_0-logloss:0.23613\n",
      "[266]\tvalidation_0-logloss:0.23594\n",
      "[267]\tvalidation_0-logloss:0.23575\n",
      "[268]\tvalidation_0-logloss:0.23557\n",
      "[269]\tvalidation_0-logloss:0.23539\n",
      "[270]\tvalidation_0-logloss:0.23521\n",
      "[271]\tvalidation_0-logloss:0.23503\n",
      "[272]\tvalidation_0-logloss:0.23485\n",
      "[273]\tvalidation_0-logloss:0.23468\n",
      "[274]\tvalidation_0-logloss:0.23451\n",
      "[275]\tvalidation_0-logloss:0.23434\n",
      "[276]\tvalidation_0-logloss:0.23418\n",
      "[277]\tvalidation_0-logloss:0.23401\n",
      "[278]\tvalidation_0-logloss:0.23385\n",
      "[279]\tvalidation_0-logloss:0.23370\n",
      "[280]\tvalidation_0-logloss:0.23354\n",
      "[281]\tvalidation_0-logloss:0.23339\n",
      "[282]\tvalidation_0-logloss:0.23324\n",
      "[283]\tvalidation_0-logloss:0.23308\n",
      "[284]\tvalidation_0-logloss:0.23293\n",
      "[285]\tvalidation_0-logloss:0.23278\n",
      "[286]\tvalidation_0-logloss:0.23264\n",
      "[287]\tvalidation_0-logloss:0.23249\n",
      "[288]\tvalidation_0-logloss:0.23234\n",
      "[289]\tvalidation_0-logloss:0.23219\n",
      "[290]\tvalidation_0-logloss:0.23205\n",
      "[291]\tvalidation_0-logloss:0.23191\n",
      "[292]\tvalidation_0-logloss:0.23177\n",
      "[293]\tvalidation_0-logloss:0.23164\n",
      "[294]\tvalidation_0-logloss:0.23150\n",
      "[295]\tvalidation_0-logloss:0.23137\n",
      "[296]\tvalidation_0-logloss:0.23124\n",
      "[297]\tvalidation_0-logloss:0.23111\n",
      "[298]\tvalidation_0-logloss:0.23098\n",
      "[299]\tvalidation_0-logloss:0.23086\n",
      "[300]\tvalidation_0-logloss:0.23073\n",
      "[301]\tvalidation_0-logloss:0.23061\n",
      "[302]\tvalidation_0-logloss:0.23049\n",
      "[303]\tvalidation_0-logloss:0.23037\n",
      "[304]\tvalidation_0-logloss:0.23025\n",
      "[305]\tvalidation_0-logloss:0.23014\n",
      "[306]\tvalidation_0-logloss:0.23002\n",
      "[307]\tvalidation_0-logloss:0.22991\n",
      "[308]\tvalidation_0-logloss:0.22980\n",
      "[309]\tvalidation_0-logloss:0.22969\n",
      "[310]\tvalidation_0-logloss:0.22958\n",
      "[311]\tvalidation_0-logloss:0.22947\n",
      "[312]\tvalidation_0-logloss:0.22936\n",
      "[313]\tvalidation_0-logloss:0.22926\n",
      "[314]\tvalidation_0-logloss:0.22916\n",
      "[315]\tvalidation_0-logloss:0.22905\n",
      "[316]\tvalidation_0-logloss:0.22895\n",
      "[317]\tvalidation_0-logloss:0.22885\n",
      "[318]\tvalidation_0-logloss:0.22876\n",
      "[319]\tvalidation_0-logloss:0.22866\n",
      "[320]\tvalidation_0-logloss:0.22856\n",
      "[321]\tvalidation_0-logloss:0.22847\n",
      "[322]\tvalidation_0-logloss:0.22838\n",
      "[323]\tvalidation_0-logloss:0.22829\n",
      "[324]\tvalidation_0-logloss:0.22820\n",
      "[325]\tvalidation_0-logloss:0.22811\n",
      "[326]\tvalidation_0-logloss:0.22802\n",
      "[327]\tvalidation_0-logloss:0.22794\n",
      "[328]\tvalidation_0-logloss:0.22785\n",
      "[329]\tvalidation_0-logloss:0.22776\n",
      "[330]\tvalidation_0-logloss:0.22768\n",
      "[331]\tvalidation_0-logloss:0.22759\n",
      "[332]\tvalidation_0-logloss:0.22752\n",
      "[333]\tvalidation_0-logloss:0.22744\n",
      "[334]\tvalidation_0-logloss:0.22735\n",
      "[335]\tvalidation_0-logloss:0.22727\n",
      "[336]\tvalidation_0-logloss:0.22719\n",
      "[337]\tvalidation_0-logloss:0.22711\n",
      "[338]\tvalidation_0-logloss:0.22703\n",
      "[339]\tvalidation_0-logloss:0.22695\n",
      "[340]\tvalidation_0-logloss:0.22688\n",
      "[341]\tvalidation_0-logloss:0.22680\n",
      "[342]\tvalidation_0-logloss:0.22672\n",
      "[343]\tvalidation_0-logloss:0.22665\n",
      "[344]\tvalidation_0-logloss:0.22657\n",
      "[345]\tvalidation_0-logloss:0.22650\n",
      "[346]\tvalidation_0-logloss:0.22643\n",
      "[347]\tvalidation_0-logloss:0.22636\n",
      "[348]\tvalidation_0-logloss:0.22629\n",
      "[349]\tvalidation_0-logloss:0.22622\n",
      "[350]\tvalidation_0-logloss:0.22615\n",
      "[351]\tvalidation_0-logloss:0.22608\n",
      "[352]\tvalidation_0-logloss:0.22602\n",
      "[353]\tvalidation_0-logloss:0.22595\n",
      "[354]\tvalidation_0-logloss:0.22589\n",
      "[355]\tvalidation_0-logloss:0.22582\n",
      "[356]\tvalidation_0-logloss:0.22576\n",
      "[357]\tvalidation_0-logloss:0.22570\n",
      "[358]\tvalidation_0-logloss:0.22564\n",
      "[359]\tvalidation_0-logloss:0.22558\n",
      "[360]\tvalidation_0-logloss:0.22552\n",
      "[361]\tvalidation_0-logloss:0.22546\n",
      "[362]\tvalidation_0-logloss:0.22540\n",
      "[363]\tvalidation_0-logloss:0.22534\n",
      "[364]\tvalidation_0-logloss:0.22528\n",
      "[365]\tvalidation_0-logloss:0.22522\n",
      "[366]\tvalidation_0-logloss:0.22516\n",
      "[367]\tvalidation_0-logloss:0.22510\n",
      "[368]\tvalidation_0-logloss:0.22505\n",
      "[369]\tvalidation_0-logloss:0.22500\n",
      "[370]\tvalidation_0-logloss:0.22494\n",
      "[371]\tvalidation_0-logloss:0.22489\n",
      "[372]\tvalidation_0-logloss:0.22484\n",
      "[373]\tvalidation_0-logloss:0.22479\n",
      "[374]\tvalidation_0-logloss:0.22473\n",
      "[375]\tvalidation_0-logloss:0.22469\n",
      "[376]\tvalidation_0-logloss:0.22464\n",
      "[377]\tvalidation_0-logloss:0.22459\n",
      "[378]\tvalidation_0-logloss:0.22454\n",
      "[379]\tvalidation_0-logloss:0.22450\n",
      "[380]\tvalidation_0-logloss:0.22445\n",
      "[381]\tvalidation_0-logloss:0.22440\n",
      "[382]\tvalidation_0-logloss:0.22436\n",
      "[383]\tvalidation_0-logloss:0.22431\n",
      "[384]\tvalidation_0-logloss:0.22427\n",
      "[385]\tvalidation_0-logloss:0.22422\n",
      "[386]\tvalidation_0-logloss:0.22418\n",
      "[387]\tvalidation_0-logloss:0.22414\n",
      "[388]\tvalidation_0-logloss:0.22409\n",
      "[389]\tvalidation_0-logloss:0.22405\n",
      "[390]\tvalidation_0-logloss:0.22401\n",
      "[391]\tvalidation_0-logloss:0.22396\n",
      "[392]\tvalidation_0-logloss:0.22392\n",
      "[393]\tvalidation_0-logloss:0.22388\n",
      "[394]\tvalidation_0-logloss:0.22384\n",
      "[395]\tvalidation_0-logloss:0.22380\n",
      "[396]\tvalidation_0-logloss:0.22376\n",
      "[397]\tvalidation_0-logloss:0.22372\n",
      "[398]\tvalidation_0-logloss:0.22369\n",
      "[399]\tvalidation_0-logloss:0.22365\n",
      "[400]\tvalidation_0-logloss:0.22361\n",
      "[401]\tvalidation_0-logloss:0.22358\n",
      "[402]\tvalidation_0-logloss:0.22354\n",
      "[403]\tvalidation_0-logloss:0.22351\n",
      "[404]\tvalidation_0-logloss:0.22347\n",
      "[405]\tvalidation_0-logloss:0.22344\n",
      "[406]\tvalidation_0-logloss:0.22341\n",
      "[407]\tvalidation_0-logloss:0.22337\n",
      "[408]\tvalidation_0-logloss:0.22335\n",
      "[409]\tvalidation_0-logloss:0.22332\n",
      "[410]\tvalidation_0-logloss:0.22329\n",
      "[411]\tvalidation_0-logloss:0.22326\n",
      "[412]\tvalidation_0-logloss:0.22323\n",
      "[413]\tvalidation_0-logloss:0.22320\n",
      "[414]\tvalidation_0-logloss:0.22317\n",
      "[415]\tvalidation_0-logloss:0.22315\n",
      "[416]\tvalidation_0-logloss:0.22312\n",
      "[417]\tvalidation_0-logloss:0.22309\n",
      "[418]\tvalidation_0-logloss:0.22307\n",
      "[419]\tvalidation_0-logloss:0.22304\n",
      "[420]\tvalidation_0-logloss:0.22301\n",
      "[421]\tvalidation_0-logloss:0.22299\n",
      "[422]\tvalidation_0-logloss:0.22297\n",
      "[423]\tvalidation_0-logloss:0.22294\n",
      "[424]\tvalidation_0-logloss:0.22292\n",
      "[425]\tvalidation_0-logloss:0.22289\n",
      "[426]\tvalidation_0-logloss:0.22287\n",
      "[427]\tvalidation_0-logloss:0.22284\n",
      "[428]\tvalidation_0-logloss:0.22282\n",
      "[429]\tvalidation_0-logloss:0.22280\n",
      "[430]\tvalidation_0-logloss:0.22277\n",
      "[431]\tvalidation_0-logloss:0.22275\n",
      "[432]\tvalidation_0-logloss:0.22273\n",
      "[433]\tvalidation_0-logloss:0.22271\n",
      "[434]\tvalidation_0-logloss:0.22268\n",
      "[435]\tvalidation_0-logloss:0.22266\n",
      "[436]\tvalidation_0-logloss:0.22264\n",
      "[437]\tvalidation_0-logloss:0.22262\n",
      "[438]\tvalidation_0-logloss:0.22260\n",
      "[439]\tvalidation_0-logloss:0.22258\n",
      "[440]\tvalidation_0-logloss:0.22256\n",
      "[441]\tvalidation_0-logloss:0.22254\n",
      "[442]\tvalidation_0-logloss:0.22252\n",
      "[443]\tvalidation_0-logloss:0.22250\n",
      "[444]\tvalidation_0-logloss:0.22248\n",
      "[445]\tvalidation_0-logloss:0.22246\n",
      "[446]\tvalidation_0-logloss:0.22244\n",
      "[447]\tvalidation_0-logloss:0.22242\n",
      "[448]\tvalidation_0-logloss:0.22241\n",
      "[449]\tvalidation_0-logloss:0.22239\n",
      "[450]\tvalidation_0-logloss:0.22237\n",
      "[451]\tvalidation_0-logloss:0.22236\n",
      "[452]\tvalidation_0-logloss:0.22234\n",
      "[453]\tvalidation_0-logloss:0.22232\n",
      "[454]\tvalidation_0-logloss:0.22230\n",
      "[455]\tvalidation_0-logloss:0.22228\n",
      "[456]\tvalidation_0-logloss:0.22227\n",
      "[457]\tvalidation_0-logloss:0.22225\n",
      "[458]\tvalidation_0-logloss:0.22224\n",
      "[459]\tvalidation_0-logloss:0.22222\n",
      "[460]\tvalidation_0-logloss:0.22220\n",
      "[461]\tvalidation_0-logloss:0.22219\n",
      "[462]\tvalidation_0-logloss:0.22217\n",
      "[463]\tvalidation_0-logloss:0.22216\n",
      "[464]\tvalidation_0-logloss:0.22214\n",
      "[465]\tvalidation_0-logloss:0.22213\n",
      "[466]\tvalidation_0-logloss:0.22211\n",
      "[467]\tvalidation_0-logloss:0.22209\n",
      "[468]\tvalidation_0-logloss:0.22208\n",
      "[469]\tvalidation_0-logloss:0.22206\n",
      "[470]\tvalidation_0-logloss:0.22205\n",
      "[471]\tvalidation_0-logloss:0.22203\n",
      "[472]\tvalidation_0-logloss:0.22202\n",
      "[473]\tvalidation_0-logloss:0.22201\n",
      "[474]\tvalidation_0-logloss:0.22199\n",
      "[475]\tvalidation_0-logloss:0.22198\n",
      "[476]\tvalidation_0-logloss:0.22197\n",
      "[477]\tvalidation_0-logloss:0.22195\n",
      "[478]\tvalidation_0-logloss:0.22194\n",
      "[479]\tvalidation_0-logloss:0.22193\n",
      "[480]\tvalidation_0-logloss:0.22191\n",
      "[481]\tvalidation_0-logloss:0.22190\n",
      "[482]\tvalidation_0-logloss:0.22189\n",
      "[483]\tvalidation_0-logloss:0.22188\n",
      "[484]\tvalidation_0-logloss:0.22186\n",
      "[485]\tvalidation_0-logloss:0.22185\n",
      "[486]\tvalidation_0-logloss:0.22184\n",
      "[487]\tvalidation_0-logloss:0.22183\n",
      "[488]\tvalidation_0-logloss:0.22181\n",
      "[489]\tvalidation_0-logloss:0.22180\n",
      "[490]\tvalidation_0-logloss:0.22179\n",
      "[491]\tvalidation_0-logloss:0.22178\n",
      "[492]\tvalidation_0-logloss:0.22177\n",
      "[493]\tvalidation_0-logloss:0.22176\n",
      "[494]\tvalidation_0-logloss:0.22174\n",
      "[495]\tvalidation_0-logloss:0.22173\n",
      "[496]\tvalidation_0-logloss:0.22172\n",
      "[497]\tvalidation_0-logloss:0.22172\n",
      "[498]\tvalidation_0-logloss:0.22170\n",
      "[499]\tvalidation_0-logloss:0.22169\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    eval_metric=\"logloss\",\n",
    "    max_depth = 10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f792ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9101072532266861\n",
      "Precision: 0.8950273016614594\n",
      "Recall: 0.9291018762692855\n",
      "F1: 0.91174633391832\n",
      "ROC AUC: 0.9101158816554326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "y_pred_prob = xgb_model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "print(\"F1:\", f1_score(y_val, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_pred_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8eb138a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=30, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=30, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=30, random_state=42)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_model = RandomForestClassifier(\n",
    "    n_estimators=100,   \n",
    "    max_depth=30,     \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da7355f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9100315094225292\n",
      "Precision: 0.8961921499707088\n",
      "Recall: 0.9274044436361432\n",
      "F1: 0.9115311852944243\n",
      "ROC AUC: 0.910039401188775\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = rfc_model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Precision:\", precision_score(y_val, y_pred))\n",
    "print(\"Recall:\", recall_score(y_val, y_pred))\n",
    "print(\"F1:\", f1_score(y_val, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_val, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89562163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding xgboost with cnn with handcrafted features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42e3b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Path to dataset files: C:\\Users\\rrpra\\.cache\\kagglehub\\datasets\\cheedcheed\\top1m\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "f= open('tld_encoding_serise_dataset_1.bin','rb')\n",
    "tld_stats = pickle.load(file=f)\n",
    "f.close()\n",
    "print(type(tld_stats))\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "# Download latest version\n",
    "folder_path = kagglehub.dataset_download(\"cheedcheed/top1m\")\n",
    "\n",
    "print(\"Path to dataset files:\", folder_path)\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.csv')]\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\"No CSV files found in the folder!\")\n",
    "\n",
    "# read the first CSV file\n",
    "file_path = os.path.join(folder_path, csv_files[0])\n",
    "alexa_top_1m_domain = pd.read_csv(file_path,header=None,names=['rank', 'domain'])\n",
    "alexa_domains_set = set(alexa_top_1m_domain['domain'].apply(str.lower))\n",
    "\n",
    "# --- Helper function: Shannon entropy ---\n",
    "def safe_parse(url: str):\n",
    "    \"\"\"Safely parse URLs, adding http:// if missing and handling bad IPv6 parts.\"\"\"\n",
    "    if not isinstance(url, str) or not url.strip():\n",
    "        return urlparse(\"http://\")  \n",
    "\n",
    "    # Ensure scheme exists\n",
    "    if not re.match(r'^[a-zA-Z]+://', url):\n",
    "        url = 'http://' + url\n",
    "\n",
    "    # Clean invalid brackets that trigger IPv6 errors\n",
    "    url = re.sub(r'\\[.*?\\]', '', url)\n",
    "\n",
    "    try:\n",
    "        return urlparse(url)\n",
    "    except ValueError:\n",
    "        # fallback: strip more aggressively if still malformed\n",
    "        url = re.sub(r'[^a-zA-Z0-9:/._\\-?&=]', '', url)\n",
    "        return urlparse(url)\n",
    "def calculate_entropy(string):\n",
    "    \"\"\"Measures randomness of characters in the URL.\"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    freq = {char: string.count(char) for char in set(string)}\n",
    "    entropy = -sum((count / len(string)) * math.log2(count / len(string)) for count in freq.values())\n",
    "    return entropy\n",
    "\n",
    "# --- Main feature extraction function ---\n",
    "def extract_handcrafted_features(url):\n",
    "    features = {}\n",
    "    if not re.match(r'^[hH]+[tT]+[tT]+[pP]+[sS]+://', url):\n",
    "        url = 'http://' + url\n",
    "    parsed = safe_parse(url)\n",
    "    \n",
    "    # 1ï¸âƒ£ Basic structural features\n",
    "    features['url_length'] = len(url)\n",
    "    features['hostname_length'] = len(parsed.netloc)\n",
    "    features['path_length'] = len(parsed.path)\n",
    "    features['num_dots'] = url.count('.')\n",
    "    features['num_hyphens'] = url.count('-')\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['num_letters'] = sum(c.isalpha() for c in url)\n",
    "    features['num_params'] = url.count('?')\n",
    "    features['num_equals'] = url.count('=')\n",
    "    features['num_slashes'] = url.count('/')\n",
    "    features['num_at'] = url.count('@')\n",
    "\n",
    "    # 2ï¸âƒ£ Lexical / composition cues\n",
    "    features['has_https'] = 1 if url.lower().startswith('https') else 0\n",
    "    features['has_ip'] = 1 if re.search(r'(\\d{1,3}\\.){3}\\d{1,3}', parsed.netloc) else 0\n",
    "    features['has_subdomain'] = 1 if parsed.netloc.count('.') > 1 else 0\n",
    "    features['has_suspicious_words'] = 1 if re.search(r'(login|secure|verify|update|free|bank|click)', url.lower()) else 0\n",
    "\n",
    "    # 3ï¸âƒ£ Domain / TLD features\n",
    "    extracted = tldextract.extract(url)\n",
    "    main_domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "    if ':' in main_domain:  # remove port\n",
    "        main_domain = main_domain.split(':')[0]\n",
    "    features['domain_length'] = len(main_domain)\n",
    "    features['in_alexa_top1m'] = 1 if main_domain in alexa_domains_set else 0\n",
    "    '''\n",
    "    if features['in_alexa_top1m'] == 0 and main_domain:  # only check if domain not in top1M\n",
    "        # find closest match in Alexa domains\n",
    "        best_match, score, _ = process.extractOne(main_domain, alexa_domains_set, scorer=fuzz.ratio)\n",
    "        features['closest_alexa_domain'] = best_match\n",
    "        features['closest_alexa_score'] = score  # 0-100\n",
    "    else:\n",
    "        features['closest_alexa_score'] = 1000  # high score to show that it is original url\n",
    "    '''\n",
    "    ext = tldextract.extract(url)\n",
    "    tld = ext.suffix    # \"com\", \"co.uk\", \"org\"\n",
    "    features['tld'] = tld if tld else 'unknown'\n",
    "    features['tld_phish_ratio'] = tld_stats['phish_ratio'].get(features['tld'], 0.5)\n",
    "    features['tld_total_frequency'] = tld_stats['total'].get(features['tld'], 1)\n",
    "\n",
    "    # 4ï¸âƒ£ Ratios\n",
    "    features['digit_ratio'] = features['num_digits'] / (features['url_length'] + 1e-5)\n",
    "    features['special_char_ratio'] = (features['num_hyphens'] + features['num_dots'] + features['num_slashes']) / (features['url_length'] + 1e-5)\n",
    "\n",
    "    # 5ï¸âƒ£ Entropy (measures randomness / obfuscation)\n",
    "    features['url_entropy'] = calculate_entropy(url)\n",
    "\n",
    "    # 6ï¸âƒ£ Misplacement indicators\n",
    "    # '@' symbol used to hide real domain (like \"http://evil.com@legit.com\")\n",
    "    features['at_in_domain'] = 1 if '@' in parsed.netloc else 0\n",
    "    \n",
    "    # Double slashes '//' appearing after path (used to trick users)\n",
    "    features['double_slash_in_path'] = 1 if re.search(r'/.+//', parsed.path) else 0\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e68a20e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 1 (Malicious URLs) train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417732/417732 [01:27<00:00, 4794.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 1 (Malicious URLs) valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:10<00:00, 4781.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 1 (Malicious URLs) test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52217/52217 [00:10<00:00, 5047.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 2 (ndarvind/phiusiil-phishing) train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 188296/188296 [00:37<00:00, 5080.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 2 (ndarvind/phiusiil-phishing) valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23537/23537 [00:04<00:00, 5824.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 2 (ndarvind/phiusiil-phishing) test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23537/23537 [00:03<00:00, 5925.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 3 (kmack/Phishing_urls) train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 528097/528097 [01:44<00:00, 5077.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 3 (kmack/Phishing_urls) valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66012/66012 [00:12<00:00, 5118.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature extracting Dataset 3 (kmack/Phishing_urls) test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66013/66013 [00:13<00:00, 5028.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "for i in encoded_data:\n",
    "\n",
    "    for split in encoded_data[i]:\n",
    "        print(\"feature extracting\", i, split)\n",
    "        encoded_data[i][split] = encoded_data[i][split].assign(**encoded_data[i][split].url.progress_apply(lambda url : pd.Series(extract_handcrafted_features(url))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09ed66ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train for Dataset 1 (Malicious URLs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [07:48:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9650496964590076\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     42808\n",
      "           1       0.97      0.83      0.90      9409\n",
      "\n",
      "    accuracy                           0.97     52217\n",
      "   macro avg       0.97      0.91      0.94     52217\n",
      "weighted avg       0.97      0.97      0.96     52217\n",
      "\n",
      "train for Dataset 2 (ndarvind/phiusiil-phishing)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [07:49:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9973233632153631\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     10052\n",
      "           1       1.00      1.00      1.00     13485\n",
      "\n",
      "    accuracy                           1.00     23537\n",
      "   macro avg       1.00      1.00      1.00     23537\n",
      "weighted avg       1.00      1.00      1.00     23537\n",
      "\n",
      "train for Dataset 3 (kmack/Phishing_urls)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [07:49:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8882778888686905\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89     33021\n",
      "           1       0.88      0.90      0.89     32991\n",
      "\n",
      "    accuracy                           0.89     66012\n",
      "   macro avg       0.89      0.89      0.89     66012\n",
      "weighted avg       0.89      0.89      0.89     66012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split features and labels\n",
    "independent_features = ['url_length', 'hostname_length', 'path_length',\n",
    "    'num_dots', 'num_hyphens', 'num_digits', 'num_letters', 'num_params',\n",
    "    'num_equals', 'num_slashes', 'num_at', 'has_https', 'has_ip',\n",
    "    'has_subdomain', 'has_suspicious_words', 'domain_length',\n",
    "    'in_alexa_top1m', 'tld_phish_ratio', 'tld_total_frequency',\n",
    "    'digit_ratio', 'special_char_ratio', 'url_entropy', 'at_in_domain',\n",
    "    'double_slash_in_path']\n",
    "dependet_features  = 'label'\n",
    "model_dict = {}\n",
    "for i in encoded_data:\n",
    "    print(f\"train for {i}\")\n",
    "    X_train = encoded_data[i]['train'][independent_features]\n",
    "    y_train = encoded_data[i]['train'][dependet_features]\n",
    "    X_test = encoded_data[i]['valid'][independent_features]\n",
    "    y_test = encoded_data[i]['valid'][dependet_features]\n",
    "\n",
    "    # Create XGBoost classifier\n",
    "    model_dict[i] = xgb.XGBClassifier(\n",
    "        n_estimators=100,      # number of boosting rounds\n",
    "        learning_rate=0.01,     # step size shrinkage\n",
    "        max_depth=30,           # tree depth\n",
    "        eval_metric='logloss', # evaluation metric\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model_dict[i].fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = model_dict[i] .predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0676fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting NN for Dataset 1 (Malicious URLs): 100%|â–ˆ| 816/816 [03:29<00:00,  3.8\n",
      "Predicting NN for Dataset 1 (Malicious URLs): 100%|â–ˆ| 102/102 [00:25<00:00,  3.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“˜ TRAIN METRICS\n",
      "Accuracy : 0.9858354160083499\n",
      "Precision: 0.9736640804283685\n",
      "Recall   : 0.9470034144202792\n",
      "F1-score : 0.9601487099011968\n",
      "ROC AUC  : 0.9971999085276664\n",
      "\n",
      "ðŸ“— VALIDATION METRICS\n",
      "Accuracy : 0.9802363215044909\n",
      "Precision: 0.9608317746726812\n",
      "Recall   : 0.928153895206717\n",
      "F1-score : 0.9442101848848524\n",
      "ROC AUC  : 0.9945813634182346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting NN for Dataset 2 (ndarvind/phiusiil-phishing): 100%|â–ˆ| 368/368 [01:32\n",
      "Predicting NN for Dataset 2 (ndarvind/phiusiil-phishing): 100%|â–ˆ| 46/46 [00:11<0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“˜ TRAIN METRICS\n",
      "Accuracy : 0.998284615711433\n",
      "Precision: 0.9970791854994501\n",
      "Recall   : 0.9999351130886169\n",
      "F1-score : 0.9985051071648409\n",
      "ROC AUC  : 0.9991190365969227\n",
      "\n",
      "ðŸ“— VALIDATION METRICS\n",
      "Accuracy : 0.9978331987933892\n",
      "Precision: 0.9963789535914869\n",
      "Recall   : 0.999851687059696\n",
      "F1-score : 0.998112299663175\n",
      "ROC AUC  : 0.9991543786916857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting NN for Dataset 3 (kmack/Phishing_urls): 100%|â–ˆ| 1032/1032 [04:15<00:0\n",
      "Predicting NN for Dataset 3 (kmack/Phishing_urls): 100%|â–ˆ| 129/129 [00:31<00:00,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“˜ TRAIN METRICS\n",
      "Accuracy : 0.9402477196424142\n",
      "Precision: 0.9342073300620742\n",
      "Recall   : 0.9471446758206784\n",
      "F1-score : 0.9406315203260139\n",
      "ROC AUC  : 0.9876472303850408\n",
      "\n",
      "ðŸ“— VALIDATION METRICS\n",
      "Accuracy : 0.8996849057747076\n",
      "Precision: 0.8908950754543568\n",
      "Recall   : 0.9108241641659847\n",
      "F1-score : 0.9007494004796163\n",
      "ROC AUC  : 0.9671546864429791\n"
     ]
    }
   ],
   "source": [
    "#logistic_regression(nn+xgboost)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "independent_features = ['url_length', 'hostname_length', 'path_length',\n",
    "    'num_dots', 'num_hyphens', 'num_digits', 'num_letters', 'num_params',\n",
    "    'num_equals', 'num_slashes', 'num_at', 'has_https', 'has_ip',\n",
    "    'has_subdomain', 'has_suspicious_words', 'domain_length',\n",
    "    'in_alexa_top1m', 'tld_phish_ratio', 'tld_total_frequency',\n",
    "    'digit_ratio', 'special_char_ratio', 'url_entropy', 'at_in_domain',\n",
    "    'double_slash_in_path']\n",
    "dependent_features  = 'label'\n",
    "logistic_model = {}\n",
    "for name in encoded_data:\n",
    "    #print(model_dict[name])\n",
    "    #print(nn_model[name])\n",
    "    nn_model[name].eval()\n",
    "    with torch.no_grad():\n",
    "        model = nn_model[name]\n",
    "        #print(model)\n",
    "        x_ = encoded_data[name]['train']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_train = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_train.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_train = torch.cat(nn_preds_train, dim=0).cpu()\n",
    "\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_train = model_dict[name].predict_proba(encoded_data[name]['train'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X = np.column_stack((nn_preds_train, xgb_preds_train))\n",
    "    meta_y = encoded_data[name]['train'][dependent_features]\n",
    "\n",
    "\n",
    "    meta_model= LogisticRegressionCV(\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    penalty=\"l2\",\n",
    "    scoring=\"roc_auc\",\n",
    "    solver=\"lbfgs\",\n",
    "    Cs=10,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_ = encoded_data[name]['valid']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_val = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_val.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_val = torch.cat(nn_preds_val, dim=0).cpu()\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_val = model_dict[name].predict_proba(encoded_data[name]['valid'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X_val = np.column_stack((nn_preds_val, xgb_preds_val))\n",
    "    meta_y_val = encoded_data[name]['valid'][dependent_features]\n",
    "\n",
    "\n",
    "    y_pred_train = meta_model.predict(meta_X)\n",
    "    y_pred_prob_train = meta_model.predict_proba(meta_X)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“˜ TRAIN METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y, y_pred_train))\n",
    "    print(\"Precision:\", precision_score(meta_y, y_pred_train))\n",
    "    print(\"Recall   :\", recall_score(meta_y, y_pred_train))\n",
    "    print(\"F1-score :\", f1_score(meta_y, y_pred_train))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y, y_pred_prob_train))\n",
    "\n",
    "    # ===============================\n",
    "    # ðŸ”¹ Validation Metrics\n",
    "    # ===============================\n",
    "    y_pred_val = meta_model.predict(meta_X_val)\n",
    "    y_pred_prob_val = meta_model.predict_proba(meta_X_val)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“— VALIDATION METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y_val, y_pred_val))\n",
    "    print(\"Precision:\", precision_score(meta_y_val, y_pred_val))\n",
    "    print(\"Recall   :\", recall_score(meta_y_val, y_pred_val))\n",
    "    print(\"F1-score :\", f1_score(meta_y_val, y_pred_val))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y_val, y_pred_prob_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334cc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting NN for Dataset 1 (Malicious URLs):  61%|â–Œ| 495/816 [02:09<01:27,  3.6"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "independent_features = ['url_length', 'hostname_length', 'path_length',\n",
    "    'num_dots', 'num_hyphens', 'num_digits', 'num_letters', 'num_params',\n",
    "    'num_equals', 'num_slashes', 'num_at', 'has_https', 'has_ip',\n",
    "    'has_subdomain', 'has_suspicious_words', 'domain_length',\n",
    "    'in_alexa_top1m', 'tld_phish_ratio', 'tld_total_frequency',\n",
    "    'digit_ratio', 'special_char_ratio', 'url_entropy', 'at_in_domain',\n",
    "    'double_slash_in_path']\n",
    "dependent_features  = 'label'\n",
    "for name in encoded_data:\n",
    "    #print(model_dict[name])\n",
    "    #print(nn_model[name])\n",
    "    nn_model[name].eval()\n",
    "    with torch.no_grad():\n",
    "        model = nn_model[name]\n",
    "        #print(model)\n",
    "        x_ = encoded_data[name]['train']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_train = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_train.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_train = torch.cat(nn_preds_train, dim=0).cpu()\n",
    "\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_train = model_dict[name].predict_proba(encoded_data[name]['train'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X = np.column_stack((nn_preds_train, xgb_preds_train))\n",
    "    meta_y = encoded_data[name]['train'][dependent_features]\n",
    "\n",
    "\n",
    "    params = {\n",
    "    \"hidden_layer_sizes\": [(8,), (16,), (24,)],\n",
    "    \"alpha\": [1e-3, 1e-4, 1e-5],  # L2 regularization\n",
    "    \"learning_rate_init\": [1e-3],\n",
    "    \"early_stopping\": [True],\n",
    "    \"max_iter\": [500],\n",
    "    }\n",
    "\n",
    "    mlp = MLPClassifier(random_state=42)\n",
    "    meta_model = GridSearchCV(mlp, params, cv=StratifiedKFold(3), scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "    meta_model.fit(meta_X, meta_y)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_ = encoded_data[name]['valid']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_val = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_val.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_val = torch.cat(nn_preds_val, dim=0).cpu()\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_val = model_dict[name].predict_proba(encoded_data[name]['valid'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X_val = np.column_stack((nn_preds_val, xgb_preds_val))\n",
    "    meta_y_val = encoded_data[name]['valid'][dependent_features]\n",
    "\n",
    "\n",
    "    y_pred_train = meta_model.predict(meta_X)\n",
    "    y_pred_prob_train = meta_model.predict_proba(meta_X)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“˜ TRAIN METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y, y_pred_train))\n",
    "    print(\"Precision:\", precision_score(meta_y, y_pred_train))\n",
    "    print(\"Recall   :\", recall_score(meta_y, y_pred_train))\n",
    "    print(\"F1-score :\", f1_score(meta_y, y_pred_train))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y, y_pred_prob_train))\n",
    "\n",
    "    # ===============================\n",
    "    # ðŸ”¹ Validation Metrics\n",
    "    # ===============================\n",
    "    y_pred_val = meta_model.predict(meta_X_val)\n",
    "    y_pred_prob_val = meta_model.predict_proba(meta_X_val)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“— VALIDATION METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y_val, y_pred_val))\n",
    "    print(\"Precision:\", precision_score(meta_y_val, y_pred_val))\n",
    "    print(\"Recall   :\", recall_score(meta_y_val, y_pred_val))\n",
    "    print(\"F1-score :\", f1_score(meta_y_val, y_pred_val))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y_val, y_pred_prob_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb309388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting NN for Dataset 3 (kmack/Phishing_urls): 100%|â–ˆ| 1032/1032 [00:34<00:0\n",
      "Predicting NN for Dataset 3 (kmack/Phishing_urls): 100%|â–ˆ| 129/129 [00:03<00:00,\n",
      "c:\\Users\\rrpra\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [09:46:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"bjective\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“˜ TRAIN METRICS\n",
      "Accuracy : 0.9488559866842644\n",
      "Precision: 0.9466781799465311\n",
      "Recall   : 0.9512442787426874\n",
      "F1-score : 0.9489557366918087\n",
      "ROC AUC  : 0.9899933244545761\n",
      "\n",
      "ðŸ“— VALIDATION METRICS\n",
      "Accuracy : 0.9139853359995153\n",
      "Precision: 0.909920456250938\n",
      "Recall   : 0.9188566578763905\n",
      "F1-score : 0.914366723976714\n",
      "ROC AUC  : 0.9709071301909017\n"
     ]
    }
   ],
   "source": [
    "#xgboost(nn+xgboost)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "independent_features = ['url_length', 'hostname_length', 'path_length',\n",
    "    'num_dots', 'num_hyphens', 'num_digits', 'num_letters', 'num_params',\n",
    "    'num_equals', 'num_slashes', 'num_at', 'has_https', 'has_ip',\n",
    "    'has_subdomain', 'has_suspicious_words', 'domain_length',\n",
    "    'in_alexa_top1m', 'tld_phish_ratio', 'tld_total_frequency',\n",
    "    'digit_ratio', 'special_char_ratio', 'url_entropy', 'at_in_domain',\n",
    "    'double_slash_in_path']\n",
    "dependent_features  = 'label'\n",
    "logistic_model = {}\n",
    "for name in encoded_data:\n",
    "    #print(model_dict[name])\n",
    "    #print(nn_model[name])\n",
    "    nn_model[name].eval()\n",
    "    with torch.no_grad():\n",
    "        model = nn_model[name]\n",
    "        #print(model)\n",
    "        x_ = encoded_data[name]['train']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_train = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_train.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_train = torch.cat(nn_preds_train, dim=0).cpu()\n",
    "\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_train = model_dict[name].predict_proba(encoded_data[name]['train'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X = np.column_stack((nn_preds_train, xgb_preds_train))\n",
    "    meta_y = encoded_data[name]['train'][dependent_features]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x_ = encoded_data[name]['valid']['encode']\n",
    "        x_np = np.stack(x_.values)\n",
    "        x_ = torch.tensor(x_np, dtype=torch.long).to(device)  # move once to GPU/CPU where model is\n",
    "        batch_size = 512  # adjust for your GPU memory\n",
    "        nn_preds_val = []\n",
    "\n",
    "        # ðŸ”¹ Process manually in batches\n",
    "        for i in tqdm(range(0, len(x_), batch_size), desc=f\"Predicting NN for {name}\", ncols=80):\n",
    "            batch_x = x_[i:i + batch_size]\n",
    "            outputs = model(batch_x)\n",
    "            nn_preds_val.append(outputs)\n",
    "\n",
    "    # ðŸ”¹ Combine all predictions\n",
    "    nn_preds_val = torch.cat(nn_preds_val, dim=0).cpu()\n",
    "    # XGBoost predictions\n",
    "    xgb_preds_val = model_dict[name].predict_proba(encoded_data[name]['valid'][independent_features])[:, 1]\n",
    "\n",
    "    # Stack predictions as new features\n",
    "    meta_X_val = np.column_stack((nn_preds_val, xgb_preds_val))\n",
    "    meta_y_val = encoded_data[name]['valid'][dependent_features]\n",
    "\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        bjective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        max_depth = 10,\n",
    "        random_state=42,\n",
    "        \n",
    "    )\n",
    "\n",
    "    xgb_model.fit(\n",
    "        meta_X, meta_y,\n",
    "        #eval_set=[(meta_X_val, meta_y_val)],\n",
    "    )\n",
    "\n",
    "\n",
    "    y_pred_train = meta_model.predict(meta_X)\n",
    "    y_pred_prob_train = meta_model.predict_proba(meta_X)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“˜ TRAIN METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y, y_pred_train))\n",
    "    print(\"Precision:\", precision_score(meta_y, y_pred_train))\n",
    "    print(\"Recall   :\", recall_score(meta_y, y_pred_train))\n",
    "    print(\"F1-score :\", f1_score(meta_y, y_pred_train))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y, y_pred_prob_train))\n",
    "\n",
    "    # ===============================\n",
    "    # ðŸ”¹ Validation Metrics\n",
    "    # ===============================\n",
    "    y_pred_val = meta_model.predict(meta_X_val)\n",
    "    y_pred_prob_val = meta_model.predict_proba(meta_X_val)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“— VALIDATION METRICS\")\n",
    "    print(\"Accuracy :\", accuracy_score(meta_y_val, y_pred_val))\n",
    "    print(\"Precision:\", precision_score(meta_y_val, y_pred_val))\n",
    "    print(\"Recall   :\", recall_score(meta_y_val, y_pred_val))\n",
    "    print(\"F1-score :\", f1_score(meta_y_val, y_pred_val))\n",
    "    print(\"ROC AUC  :\", roc_auc_score(meta_y_val, y_pred_prob_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e6e11",
   "metadata": {},
   "source": [
    "\n",
    "======================================================================  \n",
    "ðŸš€ Training model on DATASET_2 dataset\n",
    "======================================================================  \n",
    "___________ðŸ§© Using 10% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.6624, Train Acc: 0.6061 | Val Loss: 0.5799, Val Acc: 0.7219                \n",
    "Epoch 2/6 | Train Loss: 0.5179, Train Acc: 0.7594 | Val Loss: 0.5534, Val Acc: 0.7312                \n",
    "Epoch 3/6 | Train Loss: 0.4341, Train Acc: 0.8096 | Val Loss: 0.4320, Val Acc: 0.8120                \n",
    "Epoch 4/6 | Train Loss: 0.3945, Train Acc: 0.8308 | Val Loss: 0.4003, Val Acc: 0.8252              \n",
    "Epoch 5/6 | Train Loss: 0.3716, Train Acc: 0.8404 | Val Loss: 0.3567, Val Acc: 0.8496              \n",
    "Epoch 6/6 | Train Loss: 0.3588, Train Acc: 0.8458 | Val Loss: 0.3478, Val Acc: 0.8523              \n",
    "___________ðŸ§© Using 50% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.5106, Train Acc: 0.7353 | Val Loss: 0.5287, Val Acc: 0.6781              \n",
    "Epoch 2/6 | Train Loss: 0.3578, Train Acc: 0.8452 | Val Loss: 0.5693, Val Acc: 0.6529              \n",
    "Epoch 3/6 | Train Loss: 0.3324, Train Acc: 0.8569 | Val Loss: 0.5073, Val Acc: 0.7133              \n",
    "Epoch 4/6 | Train Loss: 0.3191, Train Acc: 0.8620 | Val Loss: 0.5989, Val Acc: 0.6296              \n",
    "Epoch 5/6 | Train Loss: 0.3086, Train Acc: 0.8663 | Val Loss: 0.4869, Val Acc: 0.7649              \n",
    "Epoch 6/6 | Train Loss: 0.3027, Train Acc: 0.8693 | Val Loss: 0.4661, Val Acc: 0.7953              \n",
    "__________ðŸ§© Using 100% of training data___________  \n",
    "Epoch 1/6 | Train Loss: 0.4213, Train Acc: 0.8050 | Val Loss: 0.3473, Val Acc: 0.8572              \n",
    "Epoch 2/6 | Train Loss: 0.3203, Train Acc: 0.8626 | Val Loss: 0.3535, Val Acc: 0.8393              \n",
    "Epoch 3/6 | Train Loss: 0.3016, Train Acc: 0.8692 | Val Loss: 0.4240, Val Acc: 0.7920              \n",
    "Epoch 4/6 | Train Loss: 0.2923, Train Acc: 0.8723 | Val Loss: 0.3619, Val Acc: 0.8312              \n",
    "Epoch 5/6 | Train Loss: 0.2845, Train Acc: 0.8753 | Val Loss: 0.3901, Val Acc: 0.8187              \n",
    "Epoch 6/6 | Train Loss: 0.2788, Train Acc: 0.8772 | Val Loss: 0.3979, Val Acc: 0.8223              \n",
    "\n",
    "âœ… All datasets trained successfully!  \n",
    "\n",
    "======================================================================  \n",
    "ðŸ“ˆ Final Validation Accuracy Summary  \n",
    "======================================================================  \n",
    "dataset_2            | Val Acc: 0.8223 | Val Loss: 0.3979  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf37342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c545f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_url_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m all_labels = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_url_tensor\u001b[49m), \u001b[32m64\u001b[39m):  \u001b[38;5;66;03m# batch size 64\u001b[39;00m\n\u001b[32m      7\u001b[39m         batch_x = test_url_tensor[i:i+\u001b[32m64\u001b[39m].to(device)\n\u001b[32m      8\u001b[39m         batch_y = test_labels_tensor[i:i+\u001b[32m64\u001b[39m].to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_url_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_url_tensor), 64):  # batch size 64\n",
    "        batch_x = test_url_tensor[i:i+64].to(device)\n",
    "        batch_y = test_labels_tensor[i:i+64].to(device)\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        preds = (outputs >= 0.5).long().squeeze(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        \n",
    "        # Conv blocks\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 256, kernel_size=8, padding=4)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(256, 128, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64, num_layers=1,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*2, 128)  # 64*2 because bidirectional\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)  # binary output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, embed_dim, seq_len) for Conv1d\n",
    "        \n",
    "        # Conv block 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Prepare for LSTM: (batch_size, seq_len, features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, 2*hidden_size)\n",
    "        \n",
    "        # Take the last timestep\n",
    "        x = lstm_out[:, -1, :]  # (batch_size, 128)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae29a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET1 dataset\n",
      "======================================================================\n",
      "[dataset1] Epoch 1/3 | Batch 102/102 | Loss: 0.2697, Acc: 0.8942\n",
      "[dataset1] Epoch 1/3 | Train Loss: 0.4063, Train Acc: 0.8384 | Val Loss: 0.2719, Val Acc: 0.8974\n",
      "[dataset1] Epoch 2/3 | Batch 102/102 | Loss: 0.1222, Acc: 0.9658\n",
      "[dataset1] Epoch 2/3 | Train Loss: 0.1782, Train Acc: 0.9378 | Val Loss: 0.1227, Val Acc: 0.9609\n",
      "[dataset1] Epoch 3/3 | Batch 102/102 | Loss: 0.0869, Acc: 0.9730\n",
      "[dataset1] Epoch 3/3 | Train Loss: 0.1033, Train Acc: 0.9677 | Val Loss: 0.0860, Val Acc: 0.9731\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET2 dataset\n",
      "======================================================================\n",
      "[dataset2] Epoch 1/3 | Batch 47/47 | Loss: 0.0809, Acc: 0.9864\n",
      "[dataset2] Epoch 1/3 | Train Loss: 0.3467, Train Acc: 0.8455 | Val Loss: 0.0146, Val Acc: 0.9977\n",
      "[dataset2] Epoch 2/3 | Batch 47/47 | Loss: 0.0022, Acc: 1.0000\n",
      "[dataset2] Epoch 2/3 | Train Loss: 0.0140, Train Acc: 0.9977 | Val Loss: 0.0102, Val Acc: 0.9981\n",
      "[dataset2] Epoch 3/3 | Batch 47/47 | Loss: 0.0022, Acc: 1.0000\n",
      "[dataset2] Epoch 3/3 | Train Loss: 0.0122, Train Acc: 0.9980 | Val Loss: 0.0098, Val Acc: 0.9984\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ Training CNN-BiLSTM on DATASET3 dataset\n",
      "======================================================================\n",
      "[dataset3] Epoch 1/3 | Batch 139/139 | Loss: 0.3497, Acc: 0.8435\n",
      "[dataset3] Epoch 1/3 | Train Loss: 0.4518, Train Acc: 0.7863 | Val Loss: 0.3366, Val Acc: 0.8566\n",
      "[dataset3] Epoch 2/3 | Batch 139/139 | Loss: 0.2770, Acc: 0.8722\n",
      "[dataset3] Epoch 2/3 | Train Loss: 0.3166, Train Acc: 0.8633 | Val Loss: 0.2849, Val Acc: 0.8739\n",
      "[dataset3] Epoch 3/3 | Batch 139/139 | Loss: 0.2620, Acc: 0.8855\n",
      "[dataset3] Epoch 3/3 | Train Loss: 0.2841, Train Acc: 0.8752 | Val Loss: 0.2699, Val Acc: 0.8801\n",
      "\n",
      "âœ… All datasets trained successfully!\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ Final Validation Summary (CNN-BiLSTM)\n",
      "======================================================================\n",
      "dataset1             | Val Acc: 0.9731 | Val Loss: 0.0860\n",
      "dataset2             | Val Acc: 0.9984 | Val Loss: 0.0098\n",
      "dataset3             | Val Acc: 0.8801 | Val Loss: 0.2699\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ Training Config\n",
    "# ============================================================\n",
    "num_epochs = 3\n",
    "lr = 0.001\n",
    "batch_print_interval = 1\n",
    "\n",
    "# Dictionary to store all dataset results\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ” Loop Over Each Dataset\n",
    "# ============================================================\n",
    "for dataset_name, loaders in dataloader_dict.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ðŸš€ Training CNN-BiLSTM on {dataset_name.upper()} dataset\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    train_loader = loaders[\"train_loader\"]\n",
    "    val_loader = loaders[\"val_loader\"]\n",
    "\n",
    "    model = CNN_BiLSTM(vocab_size=len(vocab)).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Track metrics\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device).float().unsqueeze(1)\n",
    "            batch_y = torch.clamp(batch_y, 0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            acc = (preds == batch_y).float().mean().item()\n",
    "\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "            correct_train += (preds == batch_y).sum().item()\n",
    "            total_train += batch_x.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % batch_print_interval == 0:\n",
    "                sys.stdout.write(f\"\\r[{dataset_name}] Epoch {epoch+1}/{num_epochs} | \"\n",
    "                                 f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                                 f\"Loss: {loss.item():.4f}, Acc: {acc:.4f}\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        avg_train_loss = train_loss / total_train\n",
    "        train_acc = correct_train / total_train\n",
    "        print()  # newline\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device).float().unsqueeze(1)\n",
    "                batch_y = torch.clamp(batch_y, 0, 1)\n",
    "\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_x.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / total_val\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"[{dataset_name}] Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # === Save results for this dataset ===\n",
    "    all_results[dataset_name] = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"final_val_acc\": val_accs[-1],\n",
    "        \"final_val_loss\": val_losses[-1]\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… All datasets trained successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ“Š Summary of Final Results\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ Final Validation Summary (CNN-BiLSTM)\")\n",
    "print(\"=\"*70)\n",
    "for name, res in all_results.items():\n",
    "    print(f\"{name:<20} | Val Acc: {res['final_val_acc']:.4f} | Val Loss: {res['final_val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aae4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
